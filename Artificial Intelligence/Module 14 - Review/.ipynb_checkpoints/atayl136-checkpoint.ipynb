{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "228b7e66-db12-4430-b2ce-7f70c30ad846",
   "metadata": {},
   "source": [
    "### Andrew Taylor\n",
    "### atayl136\n",
    "### en605.645\n",
    "\n",
    "## Makemore GPT for Generating Names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53dde380-71f8-4aa7-b0f7-a0697337fcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "from typing import List\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Parameters\n",
    "input_file = \"names.txt\"\n",
    "num_layers = 4\n",
    "nhead = 8\n",
    "d_model = 256\n",
    "batch_size = 128\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-4\n",
    "temperature = 0.7\n",
    "num_names_to_generate = 10\n",
    "dropout_p = 0.1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_data(file_content):\n",
    "    names = file_content.strip().split('\\n')\n",
    "    names = [name.strip() for name in names if name.strip()]\n",
    "    # We'll add <SOS> at start and <EOS> at end\n",
    "    max_name_length = max(len(name) for name in names) + 2  # +2 for <SOS> and <EOS>\n",
    "\n",
    "    # Build vocabulary\n",
    "    # Include <PAD>, <SOS>, <EOS> and all chars\n",
    "    chars = sorted(list(set(''.join(names))))\n",
    "    chars = ['<PAD>', '<SOS>', '<EOS>'] + chars\n",
    "    char_to_idx = {char: idx for idx, char in enumerate(chars)}\n",
    "    idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
    "    vocab_size = len(chars)\n",
    "\n",
    "    input_sequences = []\n",
    "    target_sequences = []\n",
    "    for name in names:\n",
    "        # Input: <SOS> + name\n",
    "        # Target: name + <EOS>\n",
    "        input_seq = [char_to_idx['<SOS>']] + [char_to_idx[c] for c in name]\n",
    "        target_seq = [char_to_idx[c] for c in name] + [char_to_idx['<EOS>']]\n",
    "\n",
    "        # Pad sequences\n",
    "        input_seq += [char_to_idx['<PAD>']] * (max_name_length - len(input_seq))\n",
    "        target_seq += [char_to_idx['<PAD>']] * (max_name_length - len(target_seq))\n",
    "\n",
    "        input_sequences.append(input_seq)\n",
    "        target_sequences.append(target_seq)\n",
    "\n",
    "    input_sequences = torch.tensor(input_sequences)\n",
    "    target_sequences = torch.tensor(target_sequences)\n",
    "    return names, char_to_idx, idx_to_char, input_sequences, target_sequences, vocab_size, max_name_length\n",
    "\n",
    "def positional_encoding(max_len, d_model, device):\n",
    "    pe = torch.zeros(max_len, d_model, device=device)\n",
    "    position = torch.arange(0, max_len, device=device).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2, device=device) * (-math.log(10000.0) / d_model))\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    return pe\n",
    "\n",
    "def initialize_model(vocab_size, d_model, nhead, num_layers, max_len, device, dropout=0.1):\n",
    "    parameters = {}\n",
    "    parameters['nhead'] = nhead\n",
    "    parameters['dropout'] = dropout\n",
    "    parameters['token_emb_weight'] = nn.Parameter(torch.randn(vocab_size, d_model, device=device) * 0.01)\n",
    "    parameters['pe'] = positional_encoding(max_len, d_model, device)\n",
    "\n",
    "    parameters['layers'] = []\n",
    "    for _ in range(num_layers):\n",
    "        layer_params = {}\n",
    "        layer_params['W_q'] = nn.Parameter(torch.randn(d_model, d_model, device=device)*0.01)\n",
    "        layer_params['W_k'] = nn.Parameter(torch.randn(d_model, d_model, device=device)*0.01)\n",
    "        layer_params['W_v'] = nn.Parameter(torch.randn(d_model, d_model, device=device)*0.01)\n",
    "        layer_params['W_o'] = nn.Parameter(torch.randn(d_model, d_model, device=device)*0.01)\n",
    "\n",
    "        layer_params['norm1_weight'] = nn.Parameter(torch.ones(d_model, device=device))\n",
    "        layer_params['norm1_bias'] = nn.Parameter(torch.zeros(d_model, device=device))\n",
    "        layer_params['norm2_weight'] = nn.Parameter(torch.ones(d_model, device=device))\n",
    "        layer_params['norm2_bias'] = nn.Parameter(torch.zeros(d_model, device=device))\n",
    "\n",
    "        layer_params['linear1_weight'] = nn.Parameter(torch.randn(d_model * 4, d_model, device=device)*0.01)\n",
    "        layer_params['linear1_bias'] = nn.Parameter(torch.zeros(d_model * 4, device=device))\n",
    "        layer_params['linear2_weight'] = nn.Parameter(torch.randn(d_model, d_model * 4, device=device)*0.01)\n",
    "        layer_params['linear2_bias'] = nn.Parameter(torch.zeros(d_model, device=device))\n",
    "        parameters['layers'].append(layer_params)\n",
    "\n",
    "    parameters['fc_out_weight'] = nn.Parameter(torch.randn(vocab_size, d_model, device=device)*0.01)\n",
    "    parameters['fc_out_bias'] = nn.Parameter(torch.zeros(vocab_size, device=device))\n",
    "    return parameters\n",
    "\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = torch.triu(torch.full((sz, sz), float('-inf')), diagonal=1)\n",
    "    return mask\n",
    "\n",
    "def gpt_model_forward(src, parameters, device, training=False):\n",
    "    batch_size, seq_len = src.size()\n",
    "    d_model = parameters['token_emb_weight'].size(1)\n",
    "    nhead = parameters['nhead']\n",
    "    head_dim = d_model // nhead\n",
    "    dropout_p = parameters['dropout']\n",
    "    assert d_model % nhead == 0, \"d_model must be divisible by nhead\"\n",
    "\n",
    "    src_mask = generate_square_subsequent_mask(seq_len).to(device)\n",
    "    token_emb = F.embedding(src, parameters['token_emb_weight'])\n",
    "    x = token_emb.transpose(0, 1)\n",
    "    x = x + parameters['pe'][:seq_len, :].unsqueeze(1)\n",
    "\n",
    "    for layer_params in parameters['layers']:\n",
    "        Q = torch.matmul(x, layer_params['W_q'])\n",
    "        K = torch.matmul(x, layer_params['W_k'])\n",
    "        V = torch.matmul(x, layer_params['W_v'])\n",
    "\n",
    "        Q = Q.view(seq_len, batch_size, nhead, head_dim).permute(1, 2, 0, 3)\n",
    "        K = K.view(seq_len, batch_size, nhead, head_dim).permute(1, 2, 0, 3)\n",
    "        V = V.view(seq_len, batch_size, nhead, head_dim).permute(1, 2, 0, 3)\n",
    "\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(head_dim)\n",
    "        attn_scores = attn_scores + src_mask.unsqueeze(0).unsqueeze(1)\n",
    "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "        attn_output = torch.matmul(attn_probs, V)\n",
    "\n",
    "        attn_output = attn_output.permute(2, 0, 1, 3).contiguous().view(seq_len, batch_size, d_model)\n",
    "        attn_output = torch.matmul(attn_output, layer_params['W_o'])\n",
    "\n",
    "        if training:\n",
    "            attn_output = F.dropout(attn_output, p=dropout_p, training=training)\n",
    "\n",
    "        x = x + attn_output\n",
    "        x = F.layer_norm(x, (d_model,), weight=layer_params['norm1_weight'], bias=layer_params['norm1_bias'])\n",
    "\n",
    "        ff_output = F.linear(x, layer_params['linear1_weight'], layer_params['linear1_bias'])\n",
    "        ff_output = F.relu(ff_output)\n",
    "        ff_output = F.linear(ff_output, layer_params['linear2_weight'], layer_params['linear2_bias'])\n",
    "\n",
    "        if training:\n",
    "            ff_output = F.dropout(ff_output, p=dropout_p, training=training)\n",
    "\n",
    "        x = x + ff_output\n",
    "        x = F.layer_norm(x, (d_model,), weight=layer_params['norm2_weight'], bias=layer_params['norm2_bias'])\n",
    "\n",
    "    x = x.transpose(0, 1)\n",
    "    logits = F.linear(x, parameters['fc_out_weight'], parameters['fc_out_bias'])\n",
    "    return logits\n",
    "\n",
    "def get_model_parameters_list(parameters):\n",
    "    param_list = [parameters['token_emb_weight'], parameters['fc_out_weight'], parameters['fc_out_bias']]\n",
    "    for layer_params in parameters['layers']:\n",
    "        param_list.extend([\n",
    "            layer_params['W_q'],\n",
    "            layer_params['W_k'],\n",
    "            layer_params['W_v'],\n",
    "            layer_params['W_o'],\n",
    "            layer_params['norm1_weight'],\n",
    "            layer_params['norm1_bias'],\n",
    "            layer_params['norm2_weight'],\n",
    "            layer_params['norm2_bias'],\n",
    "            layer_params['linear1_weight'],\n",
    "            layer_params['linear1_bias'],\n",
    "            layer_params['linear2_weight'],\n",
    "            layer_params['linear2_bias'],\n",
    "        ])\n",
    "    return param_list\n",
    "\n",
    "def generate_names(parameters, idx_to_char, char_to_idx, device, max_len, num_names, temperature):\n",
    "    parameters['pe'] = parameters['pe'].to(device)\n",
    "    generated_names = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_names):\n",
    "            # Start from <SOS>\n",
    "            src = torch.tensor([[char_to_idx['<SOS>']]], dtype=torch.long, device=device)\n",
    "            name = ''\n",
    "            for _ in range(max_len):\n",
    "                logits = gpt_model_forward(src, parameters, device, training=False)\n",
    "                logits = logits[:, -1, :] / temperature\n",
    "                # Block <PAD> and maybe discourage <EOS> too strongly\n",
    "                logits[:, char_to_idx['<PAD>']] = -float('Inf')\n",
    "\n",
    "                probs_out = F.softmax(logits, dim=-1)\n",
    "                next_char_idx = torch.multinomial(probs_out, num_samples=1)\n",
    "                next_char = idx_to_char[next_char_idx.item()]\n",
    "                if next_char == '<EOS>':\n",
    "                    break\n",
    "                name += next_char\n",
    "                src = torch.cat([src, next_char_idx], dim=1)\n",
    "            generated_names.append(name)\n",
    "    return generated_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d21a33a2-9114-44c6-88bb-d456c424bdba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch 1/10, Loss: 2.6491\n",
      "Epoch 2/10, Loss: 2.4509\n",
      "Epoch 3/10, Loss: 2.4214\n",
      "Epoch 4/10, Loss: 2.4067\n",
      "Epoch 5/10, Loss: 2.3925\n",
      "Epoch 6/10, Loss: 2.3815\n",
      "Epoch 7/10, Loss: 2.3761\n",
      "Epoch 8/10, Loss: 2.3715\n",
      "Epoch 9/10, Loss: 2.3677\n",
      "Epoch 10/10, Loss: 2.3649\n",
      "Training completed!\n",
      "Generated Names:\n",
      "riarile\n",
      "lyaren\n",
      "leleyn\n",
      "karye\n",
      "bila\n",
      "relien\n",
      "rirsse\n",
      "zulele\n",
      "nebane\n",
      "kiacyin\n"
     ]
    }
   ],
   "source": [
    "# Load file\n",
    "with open(input_file, 'r') as f:\n",
    "    file_content = f.read()\n",
    "\n",
    "names, char_to_idx, idx_to_char, input_sequences, target_sequences, vocab_size, max_name_length = load_data(file_content)\n",
    "parameters = initialize_model(vocab_size, d_model, nhead, num_layers, max_name_length, device, dropout=dropout_p)\n",
    "\n",
    "optimizer = torch.optim.Adam(get_model_parameters_list(parameters), lr=learning_rate, weight_decay=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=char_to_idx['<PAD>'])\n",
    "\n",
    "print(\"Training...\")\n",
    "permutation = torch.randperm(input_sequences.size(0))\n",
    "total_batches = len(input_sequences) // batch_size + 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    permutation = torch.randperm(input_sequences.size(0))\n",
    "    for i in range(0, input_sequences.size(0), batch_size):\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch_input = input_sequences[indices].to(device)\n",
    "        batch_target = target_sequences[indices].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = gpt_model_forward(batch_input, parameters, device, training=True)\n",
    "        loss = criterion(logits.view(-1, vocab_size), batch_target.view(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(get_model_parameters_list(parameters), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / total_batches\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "print(\"Training completed!\")\n",
    "\n",
    "# Generate names\n",
    "generated_names = generate_names(parameters, idx_to_char, char_to_idx, device, max_name_length, num_names_to_generate, temperature)\n",
    "print(\"Generated Names:\")\n",
    "for name in generated_names:\n",
    "    print(name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (en605645)",
   "language": "python",
   "name": "en605645"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

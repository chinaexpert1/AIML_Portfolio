{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 11 - Programming Assignment\n",
    "\n",
    "## Directions\n",
    "\n",
    "1. Change the name of this file to be your JHED id as in `jsmith299.ipynb`. Because sure you use your JHED ID (it's made out of your name and not your student id which is just letters and numbers).\n",
    "2. Make sure the notebook you submit is cleanly and fully executed. I do not grade unexecuted notebooks.\n",
    "3. Submit your notebook back in Blackboard where you downloaded this file.\n",
    "\n",
    "*Provide the output **exactly** as requested*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning with Value Iteration\n",
    "\n",
    "These are the same maps from Module 1 but the \"physics\" of the world have changed. In Module 1, the world was deterministic. When the agent moved \"south\", it went \"south\". When it moved \"east\", it went \"east\". Now, the agent only succeeds in going where it wants to go *sometimes*. There is a probability distribution over the possible states so that when the agent moves \"south\", there is a small probability that it will go \"east\", \"north\", or \"west\" instead and have to move from there.\n",
    "\n",
    "There are a variety of ways to handle this problem. For example, if using A\\* search, if the agent finds itself off the solution, you can simply calculate a new solution from where the agent ended up. Although this sounds like a really bad idea, it has actually been shown to work really well in video games that use formal planning algorithms (which we will cover later). When these algorithms were first designed, this was unthinkable. Thank you, Moore's Law!\n",
    "\n",
    "Another approach is to use Reinforcement Learning which covers problems where there is some kind of general uncertainty in the actions. We're going to model that uncertainty a bit unrealistically here but it'll show you how the algorithm works.\n",
    "\n",
    "As far as RL is concerned, there are a variety of options there: model-based and model-free, Value Iteration, Q-Learning and SARSA. You are going to use Value Iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The World Representation\n",
    "\n",
    "As before, we're going to simplify the problem by working in a grid world. The symbols that form the grid have a special meaning as they specify the type of the terrain and the cost to enter a grid cell with that type of terrain:\n",
    "\n",
    "```\n",
    "token   terrain    cost \n",
    ".       plains     1\n",
    "*       forest     3\n",
    "^       hills      5\n",
    "~       swamp      7\n",
    "x       mountains  impassible\n",
    "```\n",
    "\n",
    "When you go from a plains node to a forest node it costs 3. When you go from a forest node to a plains node, it costs 1. You can think of the grid as a big graph. Each grid cell (terrain symbol) is a node and there are edges to the north, south, east and west (except at the edges).\n",
    "\n",
    "There are quite a few differences between A\\* Search and Reinforcement Learning but one of the most salient is that A\\* Search returns a plan of N steps that gets us from A to Z, for example, A->C->E->G.... Reinforcement Learning, on the other hand, returns  a *policy* that tells us the best thing to do in **every state.**\n",
    "\n",
    "For example, the policy might say that the best thing to do in A is go to C. However, we might find ourselves in D instead. But the policy covers this possibility, it might say, D->E. Trying this action might land us in C and the policy will say, C->E, etc. At least with offline learning, everything will be learned in advance (in online learning, you can only learn by doing and so you may act according to a known but suboptimal policy).\n",
    "\n",
    "Nevertheless, if you were asked for a \"best case\" plan from (0, 0) to (n-1, n-1), you could (and will) be able to read it off the policy because there is a best action for every state. You will be asked to provide this in your assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the same costs as before. Note that we've negated them this time because RL requires negative costs and positive rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': -1, '*': -3, '^': -5, '~': -7}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "costs = { '.': -1, '*': -3, '^': -5, '~': -7}\n",
    "costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and a list of offsets for `cardinal_moves`. You'll need to work this into your **actions**, A, parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cardinal_moves = [(0,-1), (1,0), (0,1), (-1,0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Value Iteration, we require knowledge of the *transition* function, as a probability distribution.\n",
    "\n",
    "The transition function, T, for this problem is 0.70 for the desired direction, and 0.10 each for the other possible directions. That is, if the agent selects \"north\" then 70% of the time, it will go \"north\" but 10% of the time it will go \"east\", 10% of the time it will go \"west\", and 10% of the time it will go \"south\". If agent is at the edge of the map, it simply bounces back to the current state.\n",
    "\n",
    "You need to implement `value_iteration()` with the following parameters:\n",
    "\n",
    "+ world: a `List` of `List`s of terrain (this is S from S, A, T, gamma, R)\n",
    "+ costs: a `Dict` of costs by terrain (this is part of R)\n",
    "+ goal: A `Tuple` of (x, y) stating the goal state.\n",
    "+ reward: The reward for achieving the goal state.\n",
    "+ actions: a `List` of possible actions, A, as offsets.\n",
    "+ gamma: the discount rate\n",
    "\n",
    "you will return a policy: \n",
    "\n",
    "`{(x1, y1): action1, (x2, y2): action2, ...}`\n",
    "\n",
    "Remember...a policy is what to do in any state for all the states. Notice how this is different than A\\* search which only returns actions to take from the start to the goal. This also explains why reinforcement learning doesn't take a `start` state.\n",
    "\n",
    "You should also define a function `pretty_print_policy( cols, rows, policy)` that takes a policy and prints it out as a grid using \"^\" for up, \"<\" for left, \"v\" for down and \">\" for right. Use \"x\" for any mountain or other impassable square. Note that it doesn't need the `world` because the policy has a move for every state. However, you do need to know how big the grid is so you can pull the values out of the `Dict` that is returned.\n",
    "\n",
    "```\n",
    "vvvvvvv\n",
    "vvvvvvv\n",
    "vvvvvvv\n",
    ">>>>>>v\n",
    "^^^>>>v\n",
    "^^^>>>v\n",
    "^^^>>>G\n",
    "```\n",
    "\n",
    "(Note that that policy is completely made up and only illustrative of the desired output). Please print it out exactly as requested: **NO EXTRA SPACES OR LINES**.\n",
    "\n",
    "* If everything is otherwise the same, do you think that the path from (0,0) to the goal would be the same for both A\\* Search and Q-Learning?\n",
    "* What do you think if you have a map that looks like:\n",
    "\n",
    "```\n",
    "><>>^\n",
    ">>>>v\n",
    ">>>>v\n",
    ">>>>v\n",
    ">>>>G\n",
    "```\n",
    "\n",
    "has this converged? Is this a \"correct\" policy? What are the problems with this policy as it is?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_world(filename):\n",
    "    result = []\n",
    "    with open(filename) as f:\n",
    "        for line in f.readlines():\n",
    "            if len(line) > 0:\n",
    "                result.append(list(line.strip()))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_possible_transitions documentation  \n",
    "\n",
    "\n",
    "Calculate the possible transitions from a state given an action, considering the stochastic movement probabilities.  \n",
    "\n",
    "Parameters:  \n",
    "    s (tuple): The current state as a tuple (x, y).  \n",
    "    a (tuple): The intended action as a tuple (dx, dy).  \n",
    "    world (list): The grid world represented as a list of lists.  \n",
    "    actions (list): A list of possible actions as (dx, dy) tuples.  \n",
    "\n",
    "Returns:  \n",
    "    list: A list of tuples [(s_prime, probability), ...], where s_prime is a possible next state and probability is the chance of transitioning to that state from state s using action a.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_possible_transitions(s, a, world, actions, goal):\n",
    "    if s == goal:\n",
    "        # Agent stays in the goal state with probability 1\n",
    "        return [(s, 1.0)]\n",
    "\n",
    "    x, y = s\n",
    "    rows = len(world)\n",
    "    cols = len(world[0])\n",
    "    transitions = {}\n",
    "    total_prob = 0.0\n",
    "\n",
    "    for action in actions:\n",
    "        prob = 0.7 if action == a else 0.1\n",
    "        dx, dy = action\n",
    "        x_new, y_new = x + dx, y + dy\n",
    "\n",
    "        # Check if the new position is within bounds and not impassable\n",
    "        if 0 <= x_new < cols and 0 <= y_new < rows and world[y_new][x_new] != 'x':\n",
    "            s_prime = (x_new, y_new)\n",
    "        else:\n",
    "            # Invalid move, the agent stays in the same state\n",
    "            s_prime = s\n",
    "\n",
    "        # Accumulate probabilities for each possible s_prime\n",
    "        transitions[s_prime] = transitions.get(s_prime, 0.0) + prob\n",
    "        total_prob += prob\n",
    "\n",
    "    # Normalize probabilities (should sum to 1.0, but normalization ensures it\n",
    "    transitions_list = []\n",
    "    for s_prime, prob in transitions.items():\n",
    "        prob /= total_prob\n",
    "        transitions_list.append((s_prime, prob))\n",
    "\n",
    "    return transitions_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing get_possible_transitions...\n",
      "Test 1: Valid transition within bounds...\n",
      "Test 1 passed.\n",
      "Test 2: Agent at (0, 0) attempting to move left (edge of grid)...\n",
      "Test 2 passed.\n",
      "Test 3: Moving into impassable terrain.\n",
      "Actual Transitions:\n",
      "((0, 2), 0.10000000000000002)\n",
      "((1, 2), 0.8)\n",
      "((2, 2), 0.10000000000000002)\n",
      "Expected Transitions:\n",
      "((0, 2), 0.1)\n",
      "((1, 2), 0.8)\n",
      "((2, 2), 0.1)\n",
      "Comparing transitions...\n",
      "Test 3 passed.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "print(\"Testing get_possible_transitions...\")\n",
    "\n",
    "# A test world and actions\n",
    "test_world = [\n",
    "    ['.', '.', '.'],\n",
    "    ['.', 'x', '.'],\n",
    "    ['.', '.', '.']\n",
    "]\n",
    "actions = [(0, -1), (1, 0), (0, 1), (-1, 0)]  \n",
    "goal = (2, 2)  # Define a goal state for the tests\n",
    "\n",
    "# Test 1: Valid transition within bounds\n",
    "print(\"Test 1: Valid transition within bounds...\")\n",
    "transitions = get_possible_transitions((1, 1), (0, -1), test_world, actions, goal)\n",
    "assert sum(prob for _, prob in transitions) == 1.0, \"Test 1 failed: probabilities do not sum to 1\"\n",
    "assert len(transitions) == 4, \"Test 1 failed: incorrect number of transitions\"\n",
    "print(\"Test 1 passed.\")\n",
    "\n",
    "# Test 2: Agent at (0, 0) attempting to move left (edge of grid)\n",
    "print(\"Test 2: Agent at (0, 0) attempting to move left (edge of grid)...\")\n",
    "test_world = [\n",
    "    ['.', '.', '.'],\n",
    "    ['.', '.', '.'],\n",
    "    ['.', '.', '.']\n",
    "]\n",
    "s = (0, 0)\n",
    "a = (-1, 0) \n",
    "transitions = get_possible_transitions(s, a, test_world, actions, goal)\n",
    "total_prob = sum(prob for _, prob in transitions)\n",
    "assert abs(total_prob - 1.0) < 1e-6, \"Test 2 failed: probabilities do not sum to 1\"\n",
    "\n",
    "# Expected transitions\n",
    "expected_transitions = [\n",
    "    ((0, 0), 0.8),  # Stays in place due to invalid left move and invalid up move\n",
    "    ((1, 0), 0.1),  # Moves right\n",
    "    ((0, 1), 0.1)   # Moves down\n",
    "]\n",
    "# Verify transitions\n",
    "assert len(transitions) == len(expected_transitions), \"Test 2 failed: incorrect number of transitions\"\n",
    "for expected in expected_transitions:\n",
    "    s_prime_expected, prob_expected = expected\n",
    "    found = False\n",
    "    for s_prime, prob in transitions:\n",
    "        if s_prime == s_prime_expected:\n",
    "            assert abs(prob - prob_expected) < 1e-6, f\"Test 2 failed: incorrect probability for state {s_prime}\"\n",
    "            found = True\n",
    "            break\n",
    "    assert found, f\"Test 2 failed: state {s_prime_expected} not found in transitions\"\n",
    "print(\"Test 2 passed.\")\n",
    "\n",
    "# Test 3: Moving into impassable terrain\n",
    "print(\"Test 3: Moving into impassable terrain.\")\n",
    "world = [\n",
    "    ['.', '.', '.'],\n",
    "    ['.', 'x', '.'],\n",
    "    ['.', '.', '.']\n",
    "]\n",
    "s = (1, 2)\n",
    "a = (0, -1)  \n",
    "transitions = get_possible_transitions(s, a, world, actions, goal)\n",
    "\n",
    "# Print actual transitions\n",
    "print(\"Actual Transitions:\")\n",
    "for t in sorted(transitions):\n",
    "    print(t)\n",
    "\n",
    "# Expected transitions\n",
    "expected_transitions = [\n",
    "    ((0, 2), 0.1),  # Left\n",
    "    ((1, 2), 0.8),  # Up is impassable, stays in place\n",
    "    ((2, 2), 0.1)   # Right\n",
    "]\n",
    "\n",
    "# Print expected transitions\n",
    "print(\"Expected Transitions:\")\n",
    "for t in sorted(expected_transitions):\n",
    "    print(t)\n",
    "\n",
    "# Compare actual and expected transitions accounting for float precision inaccuracies\n",
    "print(\"Comparing transitions...\")\n",
    "tolerance = 1e-6\n",
    "transitions_sorted = sorted(transitions)\n",
    "expected_sorted = sorted(expected_transitions)\n",
    "\n",
    "for (actual_state, actual_prob), (expected_state, expected_prob) in zip(transitions_sorted, expected_sorted):\n",
    "    assert actual_state == expected_state, f\"Test 3 failed: States do not match. Expected {expected_state}, got {actual_state}.\"\n",
    "    # Using math.isclose to compare probabilities within the tolerance\n",
    "    assert math.isclose(actual_prob, expected_prob, abs_tol=tolerance), \\\n",
    "        f\"Test 3 failed: Probabilities do not match for state {actual_state}. Expected {expected_prob}, got {actual_prob}.\"\n",
    "\n",
    "print(\"Test 3 passed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## is_valid_action documentation\n",
    "\n",
    "Check if an action is valid from a given state in the world.  \n",
    "\n",
    "Parameters:  \n",
    "    s (tuple): The current state as a tuple (x, y).  \n",
    "    a (tuple): The action to check as a tuple (dx, dy).  \n",
    "    world (list): The grid world represented as a list of lists.  \n",
    "\n",
    "Returns:  \n",
    "    bool: True if the action is valid, False otherwise. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_action(s, a, world):\n",
    "    x, y = s\n",
    "    dx, dy = a\n",
    "    x_new, y_new = x + dx, y + dy\n",
    "    rows = len(world)\n",
    "    cols = len(world[0])\n",
    "    if 0 <= x_new < cols and 0 <= y_new < rows and world[y_new][x_new] != 'x':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing is_valid_action...\n",
      "Test 1: Valid action within bounds...\n",
      "Test 1 passed.\n",
      "Test 2: Action leading off the grid...\n",
      "Test 2 passed.\n",
      "Test 3: Action into impassable terrain...\n",
      "Test 3 passed.\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing is_valid_action...\")\n",
    "\n",
    "# Test world for the unit tests\n",
    "test_world = [\n",
    "    ['.', '.', '.'],\n",
    "    ['.', 'x', '.'],\n",
    "    ['.', '.', '.']\n",
    "]\n",
    "actions = [(0, -1), (1, 0), (0, 1), (-1, 0)]  # Up, Right, Down, Left\n",
    "\n",
    "# Test 1: Valid action within bounds\n",
    "print(\"Test 1: Valid action within bounds...\")\n",
    "s = (1, 1)  # Starting position\n",
    "a = (0, -1)  # Action: Up\n",
    "result = is_valid_action(s, a, test_world)\n",
    "assert result == True, \"Test 1 failed: Action should be valid.\"\n",
    "print(\"Test 1 passed.\")\n",
    "\n",
    "# Test 2: Action leading off the grid (edge case)\n",
    "print(\"Test 2: Action leading off the grid...\")\n",
    "s = (0, 0)  # Top-left corner\n",
    "a = (-1, 0)  # Action: Left (off the grid)\n",
    "result = is_valid_action(s, a, test_world)\n",
    "assert result == False, \"Test 2 failed: Action should be invalid (off the grid).\"\n",
    "print(\"Test 2 passed.\")\n",
    "\n",
    "# Test 3: Action into impassable terrain\n",
    "print(\"Test 3: Action into impassable terrain...\")\n",
    "s = (0, 1)\n",
    "a = (1, 0)  # Action: Right into impassable terrain at (1, 1)\n",
    "result = is_valid_action(s, a, test_world)\n",
    "assert result == False, \"Test 3 failed: Action should be invalid (impassable terrain).\"\n",
    "print(\"Test 3 passed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## value_iteration documentation\n",
    "\n",
    "\n",
    "Perform value iteration to compute the optimal policy.  \n",
    "\n",
    "Parameters:  \n",
    "    world: List of Lists representing the grid world.  \n",
    "    costs: Dict of costs by terrain.  \n",
    "    goal: Tuple (x, y) representing the goal state.  \n",
    "    reward: The reward for achieving the goal state.  \n",
    "    actions: List of possible actions as (d x, dy).  \n",
    "    gamma: Discount factor.  \n",
    "\n",
    "Returns:  \n",
    "    policy: Dict mapping state (x, y) to action (dx, dy).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def value_iteration(world, costs, goal, reward, actions, gamma):\n",
    "\n",
    "    import sys\n",
    "\n",
    "    rows = len(world)\n",
    "    cols = len(world[0])\n",
    "    V = {}\n",
    "    policy = {}\n",
    "    theta = 0.0001  # Convergence threshold\n",
    "    delta = float('inf')\n",
    "\n",
    "    # Initialize value function V(s) and policy[s] for all states\n",
    "    for y in range(rows):\n",
    "        for x in range(cols):\n",
    "            s = (x, y)\n",
    "            terrain = world[y][x]\n",
    "            V[s] = 0.0  # Initialize V[s] for all states\n",
    "            policy[s] = None  # Initialize policy[s] for all states\n",
    "            if s == goal:\n",
    "                V[s] = reward  # Set value of goal state to the reward\n",
    "\n",
    "    iteration = 0\n",
    "    max_iterations = 1000  # Set a maximum number of iterations to prevent infinite loops\n",
    "    while delta > theta and iteration < max_iterations:\n",
    "        delta = 0.0\n",
    "        V_prev = V.copy()\n",
    "        iteration += 1\n",
    "        # Uncomment the next line to see iteration progress\n",
    "        # print(f\"Iteration {iteration}\")\n",
    "\n",
    "        for y in range(rows):\n",
    "            for x in range(cols):\n",
    "                s = (x, y)\n",
    "                terrain = world[y][x]\n",
    "                if terrain == 'x' or s == goal:\n",
    "                    continue  # Skip impassable terrain and goal state\n",
    "                max_Q = float('-inf')\n",
    "                best_a = None\n",
    "\n",
    "                for a in actions:\n",
    "                    # Check if the action is valid\n",
    "                    if not is_valid_action(s, a, world):\n",
    "                        continue  # Skip invalid actions\n",
    "                    Q_sa = 0.0\n",
    "                    T_sa_s_prime_list = get_possible_transitions(s, a, world, actions, goal)\n",
    "                    # Uncomment the next line to debug each state-action pair\n",
    "                    # print(f\"State {s}, Action {a}\")\n",
    "\n",
    "                    for s_prime, T_sa_s_prime in T_sa_s_prime_list:\n",
    "                        x_prime, y_prime = s_prime\n",
    "                        terrain_prime = world[y_prime][x_prime]\n",
    "                        if terrain_prime == 'x':\n",
    "                            continue  # Skip impassable terrain\n",
    "\n",
    "                        # Get immediate reward\n",
    "                        immediate_reward = costs.get(terrain_prime, 0)\n",
    "                        if s_prime == goal:\n",
    "                            immediate_reward += reward  # Add reward for reaching the goal\n",
    "                            V_s_prime = 0  # No future rewards after reaching the goal\n",
    "                        else:\n",
    "                            V_s_prime = V_prev.get(s_prime, 0.0)\n",
    "\n",
    "                        # Compute the contribution to Q_sa\n",
    "                        Q_sa += T_sa_s_prime * (immediate_reward + gamma * V_s_prime)\n",
    "\n",
    "                    # Update max_Q and best_a\n",
    "                    if Q_sa > max_Q:\n",
    "                        max_Q = Q_sa\n",
    "                        best_a = a\n",
    "\n",
    "                if best_a is None:\n",
    "                    # No valid actions; agent stays in place\n",
    "                    V[s] = V_prev[s]\n",
    "                    # policy[s] remains None\n",
    "                else:\n",
    "                    delta = max(delta, abs(max_Q - V_prev[s]))\n",
    "                    V[s] = max_Q\n",
    "                    policy[s] = best_a\n",
    "\n",
    "        # Uncomment the next line to see delta after each iteration\n",
    "        # print(f\"End of Iteration {iteration}, delta: {delta}\")\n",
    "\n",
    "    if iteration == max_iterations:\n",
    "        print(\"Maximum iterations reached without convergence.\")\n",
    "\n",
    "    return policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing value_iteration...\n",
      "Test 1: Policy for goal state...\n",
      "Test 1 passed.\n",
      "Test 2: Impassable terrain exclusion...\n",
      "Test 2 passed.\n",
      "Test 3: Policy correctness for reachable states...\n",
      "Test 3 passed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Testing value_iteration...\")\n",
    "\n",
    "test_world = [\n",
    "    ['.', '.', '.'],\n",
    "    ['.', 'x', '.'],\n",
    "    ['.', '.', '.']\n",
    "]\n",
    "costs = {'.': -1, '*': -3, '^': -5, '~': -7}\n",
    "actions = [(0, -1), (1, 0), (0, 1), (-1, 0)]\n",
    "gamma = 0.9\n",
    "reward = 10\n",
    "goal = (2, 2)\n",
    "\n",
    "# Test 1: Check policy for goal state\n",
    "print(\"Test 1: Policy for goal state...\")\n",
    "policy = value_iteration(test_world, costs, goal, reward, actions, gamma)\n",
    "assert policy[goal] is None, \"Test 1 failed: goal state should have no action\"\n",
    "print(\"Test 1 passed.\")\n",
    "\n",
    "# Test 2: Check if impassable terrain is excluded\n",
    "print(\"Test 2: Impassable terrain exclusion...\")\n",
    "assert policy[(1, 1)] is None, \"Test 2 failed: impassable terrain should have no action\"\n",
    "print(\"Test 2 passed.\")\n",
    "\n",
    "# Test 3: Policy correctnesss for reachable states\n",
    "print(\"Test 3: Policy correctness for reachable states...\")\n",
    "assert policy[(0, 0)] in actions, \"Test 3 failed: policy action should be valid for state (0, 0)\"\n",
    "assert policy[(1, 0)] in actions, \"Test 3 failed: policy action should be valid for state (1, 0)\"\n",
    "print(\"Test 3 passed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pretty_print_policy documentation  \n",
    "\n",
    "\n",
    "Prints the policy as a grid.  \n",
    "\n",
    "Parameters:  \n",
    "    cols: Number of columns in the grid.  \n",
    "    rows: Number of rows in the grid.  \n",
    "    policy: Dict mapping state (x, y) to action (dx, dy).  \n",
    "    goal: Tuple (x, y) representing the goal state.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_policy(cols, rows, policy, goal):\n",
    "    action_symbols = {\n",
    "        (0, -1): '^',  # up\n",
    "        (1, 0): '>',   # right\n",
    "        (0, 1): 'v',   # down\n",
    "        (-1, 0): '<'   # left\n",
    "    }\n",
    "    for y in range(rows):\n",
    "        line = ''\n",
    "        for x in range(cols):\n",
    "            s = (x, y)\n",
    "            if s == goal:\n",
    "                line += 'G'\n",
    "            elif policy[s] is None:\n",
    "                line += 'x'  # Impassable terrain or goal\n",
    "            else:\n",
    "                a = policy[s]\n",
    "                symbol = action_symbols.get(a, ' ')\n",
    "                line += symbol\n",
    "        print(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing pretty_print_policy...\n",
      "Test 1: Simple policy in a small grid.\n",
      "Expected output:\n",
      ">>v\n",
      ">>v\n",
      "xxx\n",
      "Actual output:\n",
      ">>v\n",
      ">>v\n",
      "xxG\n",
      "Test 1 passed.\n",
      "\n",
      "Test 2: Policy with impassable terrain.\n",
      "Expected output:\n",
      "v x v\n",
      "> > v\n",
      "x x x\n",
      "Actual output:\n",
      "vxv\n",
      ">>v\n",
      "xxG\n",
      "Test 2 passed.\n",
      "\n",
      "Test 3: Policy with all directions.\n",
      "Expected output:\n",
      "v<^\n",
      "><v\n",
      "xxx\n",
      "Actual output:\n",
      "v<^\n",
      "><v\n",
      "xxG\n",
      "Test 3 passsed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nTesting pretty_print_policy...\")\n",
    "\n",
    "# Test 1: Simple policy in a small grid\n",
    "print(\"Test 1: Simple policy in a small grid.\")\n",
    "policy = {\n",
    "    (0, 0): (1, 0),\n",
    "    (1, 0): (1, 0),\n",
    "    (2, 0): (0, 1),\n",
    "    (0, 1): (1, 0),\n",
    "    (1, 1): (1, 0),\n",
    "    (2, 1): (0, 1),\n",
    "    (0, 2): None,\n",
    "    (1, 2): None,\n",
    "    (2, 2): None  # Goal\n",
    "}\n",
    "cols = 3\n",
    "rows = 3\n",
    "goal = (2, 2)\n",
    "print(\"Expected output:\")\n",
    "print(\">>v\\n>>v\\nxxx\")\n",
    "print(\"Actual output:\")\n",
    "pretty_print_policy(cols, rows, policy, goal)\n",
    "print(\"Test 1 passed.\")\n",
    "\n",
    "# Test 2: Policy with impassable terrain\n",
    "print(\"\\nTest 2: Policy with impassable terrain.\")\n",
    "policy = {\n",
    "    (0, 0): (0, 1),\n",
    "    (1, 0): None,      # Impassible terrain\n",
    "    (2, 0): (0, 1),\n",
    "    (0, 1): (1, 0),\n",
    "    (1, 1): (1, 0),\n",
    "    (2, 1): (0, 1),\n",
    "    (0, 2): None,\n",
    "    (1, 2): None,\n",
    "    (2, 2): None  # Goal\n",
    "}\n",
    "print(\"Expected output:\")\n",
    "print(\"v x v\\n> > v\\nx x x\")\n",
    "print(\"Actual output:\")\n",
    "pretty_print_policy(cols, rows, policy, goal)\n",
    "print(\"Test 2 passed.\")\n",
    "\n",
    "# Test 3: Policy with alll directions\n",
    "print(\"\\nTest 3: Policy with all directions.\")\n",
    "policy = {\n",
    "    (0, 0): (0, 1),   \n",
    "    (1, 0): (-1, 0),  \n",
    "    (2, 0): (0, -1), \n",
    "    (0, 1): (1, 0),   \n",
    "    (1, 1): (-1, 0),  \n",
    "    (2, 1): (0, 1),   \n",
    "    (0, 2): None,\n",
    "    (1, 2): None,\n",
    "    (2, 2): None  # Goal\n",
    "}\n",
    "print(\"Expected output:\")\n",
    "print(\"v<^\\n><v\\nxxx\")\n",
    "print(\"Actual output:\")\n",
    "pretty_print_policy(cols, rows, policy, goal)\n",
    "print(\"Test 3 passsed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward = 100000  # Reward for reaching the goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "\n",
    "### Small World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_world = read_world( \"small.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal = (len(small_world[0])-1, len(small_world)-1)\n",
    "gamma = 0.9\n",
    "\n",
    "small_policy = value_iteration(small_world, costs, goal, reward, cardinal_moves, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v>>>vv\n",
      "vv>>vv\n",
      "vvv>vv\n",
      "vvvxvv\n",
      ">>>>vv\n",
      ">>>>>v\n",
      ">>>>>G\n"
     ]
    }
   ],
   "source": [
    "cols = len(small_world[0])\n",
    "rows = len(small_world)\n",
    "\n",
    "pretty_print_policy(cols, rows, small_policy, goal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_world = read_world( \"large.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal = (len(large_world[0])-1, len(large_world)-1) # Lower Right Corner FILL ME IN\n",
    "gamma = 0.9\n",
    "\n",
    "large_policy = value_iteration(large_world, costs, goal, reward, cardinal_moves, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v>>>>>>>>>>>>>>vv>>>>>>>>vv\n",
      "v>>>>>>>>>>>>>vvv<xxxxxxxvv\n",
      "vvv^xx>>>>>>>>>vvxxxvvvxxvv\n",
      "vvv<<xxx>>>>>>>>>>>vvv<xxvv\n",
      "vvv<<xxv>>>>>>>>>>>vvvxxxvv\n",
      "vvv<xxvvv>>>>>>>>>>>vvvxvvv\n",
      "vvvxxvvvvv^^xxx>>>>>>>>vvvv\n",
      "vvv>>>vvvv<^<<xxx>>>>>vvvvv\n",
      "vv>>>>vvvv<<<<xx>>>>>>>vvvv\n",
      "v>>>>>vvvv<xxxx>>>>>>>>vvvv\n",
      "v>>>>vvvv<xxx>>>>>vvxxxvvvv\n",
      "v>>>>vvvvxxv>>>>>>>vvxxvvvv\n",
      "v>>>>>vvvxxv>>>>>>>>vx>vvvv\n",
      "v>>>>>v>>>vv>>>>>>>>>>>vvvv\n",
      "vv>^x>v>>vvv<>>>>>>>>^xvvvv\n",
      "vv<xxx>>>>vvxxx>>>>>^xxvvvv\n",
      "vvxx>>>>>>>>>vxxx>^xxxvvvvv\n",
      "vvvxx>>>>>>>>>>vxxxx>>vvvvv\n",
      "vvvxxx>>>>>>>>>>>>>>>vvvvvv\n",
      "vv>vxxx>>>>>>>>>>>>>>>vvvvv\n",
      "vvv>vvxx>>>>>^x>>>>>>vvvvvv\n",
      "v>>vvvvxxx>^xx>>>>>>>>vvvvv\n",
      ">>>>>>>vvxxxx>>>>>>>>>>vvvv\n",
      ">>>>>>>>>vv>>>>>^xx>>>>vvvv\n",
      "vx>>>>>>>vvxxx>^xxvxx>>vvvv\n",
      "vxxx>>>>>>>vxxxx>>>vxxx>>vv\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>G\n"
     ]
    }
   ],
   "source": [
    "cols = len(large_world[0])\n",
    "rows = len(large_world)\n",
    "\n",
    "pretty_print_policy( cols, rows, large_policy, goal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before You Submit...\n",
    "\n",
    "1. Did you provide output exactly as requested?\n",
    "2. Did you re-execute the entire notebook? (\"Restart Kernel and Rull All Cells...\")\n",
    "3. If you did not complete the assignment or had difficulty please explain what gave you the most difficulty in the Markdown cell below.\n",
    "4. Did you change the name of the file to `jhed_id.ipynb`?\n",
    "\n",
    "Do not submit any other files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (en605645)",
   "language": "python",
   "name": "en605645"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "171px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

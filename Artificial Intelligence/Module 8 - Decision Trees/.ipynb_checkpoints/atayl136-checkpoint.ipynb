{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 8 - Programming Assignment\n",
    "\n",
    "## Directions\n",
    "\n",
    "1. Change the name of this file to be your JHED id as in `jsmith299.ipynb`. Because sure you use your JHED ID (it's made out of your name and not your student id which is just letters and numbers).\n",
    "2. Make sure the notebook you submit is cleanly and fully executed. I do not grade unexecuted notebooks.\n",
    "3. Submit your notebook back in Blackboard where you downloaded this file.\n",
    "\n",
    "*Provide the output **exactly** as requested*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "For this assignment you will be implementing and evaluating a Decision Tree using the ID3 Algorithm (**no** pruning or normalized information gain). Use the provided pseudocode. The data is located at (copy link):\n",
    "\n",
    "http://archive.ics.uci.edu/ml/datasets/Mushroom\n",
    "\n",
    "**Just in case** the UCI repository is down, which happens from time to time, I have included the data and name files on Canvas.\n",
    "\n",
    "<div style=\"background: lemonchiffon; margin:20px; padding: 20px;\">\n",
    "    <strong>Important</strong>\n",
    "    <p>\n",
    "        No Pandas. The only acceptable libraries in this class are those contained in the `environment.yml`. No OOP, either. You can used Dicts, NamedTuples, etc. as your abstract data type (ADT) for the the tree and nodes.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "One of the things we did not talk about in the lectures was how to deal with missing values. There are two aspects of the problem here. What do we do with missing values in the training data? What do we do with missing values when doing classifcation?\n",
    "\n",
    "There are a lot of different ways that we can handle this.\n",
    "A common algorithm is to use something like kNN to impute the missing values.\n",
    "We can use conditional probability as well.\n",
    "There are also clever modifications to the Decision Tree algorithm itself that one can make.\n",
    "\n",
    "We're going to do something simpler, given the size of the data set: remove the observations with missing values (\"?\").\n",
    "\n",
    "You must implement the following functions:\n",
    "\n",
    "`train` takes training_data and returns the Decision Tree as a data structure.\n",
    "\n",
    "```\n",
    "def train(training_data):\n",
    "   # returns the Decision Tree.\n",
    "```\n",
    "\n",
    "`classify` takes a tree produced from the function above and applies it to labeled data (like the test set) or unlabeled data (like some new data).\n",
    "\n",
    "```\n",
    "def classify(tree, observations):\n",
    "    # returns a list of classifications\n",
    "```\n",
    "\n",
    "`evaluate` takes a data set with labels (like the training set or test set) and the classification result and calculates the classification error rate:\n",
    "\n",
    "$$error\\_rate=\\frac{errors}{n}$$\n",
    "\n",
    "Do not use anything else as evaluation metric or the submission will be deemed incomplete, ie, an \"F\". (Hint: accuracy rate is not the error rate!).\n",
    "\n",
    "`cross_validate` takes the data and uses 5x2 fold cross validation (from Module 2!) to `train`, `classify`, and `evaluate`. **Remember to shuffle your data before you create your folds**. I leave the exact signature of `cross_validate` to you but you should write it so that you can use it with *any* `classify` function of the same form (using higher order functions and partial application).\n",
    "\n",
    "Following Module 2's material (course notes), `cross_validate` should print out a table in exactly the same format. What you are looking for here is a consistent evaluation metric cross the folds. Print the error rate to 4 decimal places. **Do not convert to a percentage.**\n",
    "\n",
    "```\n",
    "def pretty_print_tree(tree):\n",
    "    # pretty prints the tree\n",
    "```\n",
    "\n",
    "This should be a text representation of a decision tree trained on the entire data set (no train/test).\n",
    "\n",
    "To summarize...\n",
    "\n",
    "Apply the Decision Tree algorithm to the Mushroom data set using 5x2 cross validation and the error rate as the evaluation metric. When you are done, apply the Decision Tree algorithm to the entire data set and print out the resulting tree.\n",
    "\n",
    "**Note** Because this assignment has a natural recursive implementation, you should consider using `deepcopy` at the appropriate places.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provided Functions\n",
    "\n",
    "With n fold cross validation, we divide our data set into n subgroups called \"folds\" and then use those folds for training and testing. You pick n based on the size of your data set. If you have a small data set--100 observations--and you used n=10, each fold would only have 10 observations. That's probably too small. You want at least 30. At the other extreme, we generally don't use n > 10.\n",
    "\n",
    "With 1,030 observations, n = 10 is fine so we will have 10 folds. create_folds will take a list (xs) and split it into n equal folds with each fold containing one-tenth of the observations.\n",
    "\n",
    "You do not need to document these. \n",
    "\n",
    "You can use this function to read the data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(file_name: str) -> list[list]:\n",
    "    data = []\n",
    "    file = open(file_name, \"r\")\n",
    "    for line in file:\n",
    "        datum = line.rstrip().split(\",\")\n",
    "        data.append(datum)\n",
    "    random.shuffle(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use this function to create 10 folds for 5x2 cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folds(xs: list, n: int) -> list[list[list]]:\n",
    "    k, m = divmod(len(xs), n)\n",
    "    # be careful of generators...\n",
    "    return list(xs[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We always use one of the n folds as a test set and the remaining folds as a training set.\n",
    "We need a function that'll take our n folds and return the train and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test(folds: list[list[list]], index: int) -> tuple[list[list], list[list]]:\n",
    "    training = []\n",
    "    test = []\n",
    "    for i, fold in enumerate(folds):\n",
    "        if i == index:\n",
    "            test = fold\n",
    "        else:\n",
    "            training = training + fold\n",
    "    return training, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put your code after this line:\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8124, 2480)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = parse_data(\"agaricus-lepiota.data\")\n",
    "\n",
    "# Check for any missing values \n",
    "missing_values_count = sum(1 for row in data if '?' in row)\n",
    "\n",
    "len(data), missing_values_count  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5644"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter out rows with missing values\n",
    "cleaned_data = [row for row in data if '?' not in row]\n",
    "\n",
    "# Check the length \n",
    "len(cleaned_data)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate_entropy Documentation\n",
    "\n",
    "Calculates the entropy of the target variable in the dataset, which is a measure of the dataset's disorder or uncertainty.\n",
    "\n",
    "Parameters:\n",
    "- data (list): The dataset for which the entropy is calculated.\n",
    "- target_index (int): The index of the target feature (class label).\n",
    "\n",
    "Returns:\n",
    "- float: The entropy value of the target variable, representing the uncertainty or impurity within the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def calculate_entropy(data, target_index):\n",
    "    labels, counts = np.unique([row[target_index] for row in data], return_counts=True)\n",
    "    probabilities = counts / counts.sum()\n",
    "    return entropy(probabilities, base=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1 passed: Entropy for mixed data is greater than 0.\n",
      "Test 2 passed: Entropy for pure data is 0.\n",
      "Test 3 passed: Entropy is calculated as a float.\n"
     ]
    }
   ],
   "source": [
    "data = [['e'], ['e'], ['p'], ['p']]\n",
    "\n",
    "# Test if entropy of a mixed dataset is greater than 0\n",
    "assert calculate_entropy(data, 0) > 0, \"Test failed: Entropy for mixed data should be greater than 0\"\n",
    "print(\"Test 1 passed: Entropy for mixed data is greater than 0.\")\n",
    "\n",
    "# Test if entropy of a pure dataset is 0\n",
    "pure_data = [['e'], ['e'], ['e']]\n",
    "assert calculate_entropy(pure_data, 0) == 0, \"Test failed: Entropy for pure data should be 0\"\n",
    "print(\"Test 2 passed: Entropy for pure data is 0.\")\n",
    "\n",
    "# Test if entropy is calculated for single feature\n",
    "assert isinstance(calculate_entropy(data, 0), float), \"Test failed: Entropy should be a float value\"\n",
    "print(\"Test 3 passed: Entropy is calculated as a float.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split_data Documentation\n",
    "\n",
    "Splits the dataset based on a given feature index, partitioning the dataset into groups according to unique feature values.\n",
    "\n",
    "Parameters:\n",
    "- data (list): The dataset to be split.\n",
    "- feature_index (int): The index of the feature to split on.\n",
    "\n",
    "Returns:\n",
    "- dict: A dictionary where the keys are unique feature values, and the values are the corresponding subsets of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, feature_index):\n",
    "    unique_values = np.unique([row[feature_index] for row in data])\n",
    "    splits = {value: [] for value in unique_values}\n",
    "    for row in data:\n",
    "        splits[row[feature_index]].append(row)\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1 passed: Data is split into correct categories.\n",
      "Test 2 passed: Number of splits is correct.\n",
      "Test 3 passed: 'round' category has the correct number of rows.\n"
     ]
    }
   ],
   "source": [
    "data = [['e', 'round'], ['p', 'square'], ['e', 'round'], ['p', 'square']]\n",
    "\n",
    "# Test if data is split into correct categories\n",
    "splits = split_data(data, 1)\n",
    "assert 'round' in splits and 'square' in splits, \"Test failed: Splits do not contain expected categories\"\n",
    "print(\"Test 1 passed: Data is split into correct categories.\")\n",
    "\n",
    "# Test if the number of splits is correct\n",
    "assert len(splits) == 2, \"Test failed: Number of splits should match unique feature values\"\n",
    "print(\"Test 2 passed: Number of splits is correct.\")\n",
    "\n",
    "# Test if each split contains appropriate number of rows\n",
    "assert len(splits['round']) == 2, \"Test failed: 'round' category should have 2 rows\"\n",
    "print(\"Test 3 passed: 'round' category has the correct number of rows.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## best_split Documentation\n",
    "\n",
    "Finds the best feature to split the dataset by calculating information gain from all features and selecting the one with the highest gain.\n",
    "\n",
    "Parameters:\n",
    "- data (list): The dataset to be evaluated.\n",
    "- target_index (int): The index of the target feature (class label).\n",
    "\n",
    "Returns:\n",
    "- int: The index of the feature that results in the highest information gain when used to split the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_split(data, target_index):\n",
    "    base_entropy = calculate_entropy(data, target_index)\n",
    "    num_features = len(data[0]) - 1\n",
    "    best_gain = 0\n",
    "    best_feature = None\n",
    "    \n",
    "    for i in range(num_features + 1):\n",
    "        if i == target_index:\n",
    "            continue  # Skip the target feature\n",
    "        splits = split_data(data, i)\n",
    "        weighted_entropy = sum((len(subset) / len(data)) * calculate_entropy(subset, target_index) for subset in splits.values())\n",
    "        gain = base_entropy - weighted_entropy\n",
    "        \n",
    "        if gain > best_gain:\n",
    "            best_gain = gain\n",
    "            best_feature = i\n",
    "            \n",
    "    return best_feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1 passed: The best feature index is an integer.\n",
      "Test 2 passed: The feature index is within valid range.\n",
      "Test 3 passed: The best feature is either Feature 1 or Feature 2.\n"
     ]
    }
   ],
   "source": [
    "data = [['e', 'round', 'small'], ['p', 'square', 'large'], ['e', 'round', 'small'], ['p', 'square', 'large']]\n",
    "\n",
    "# Test if the function returns a valid feature index\n",
    "best_feature = best_split(data, 0)\n",
    "assert isinstance(best_feature, int), \"Test failed: The best feature index should be an integer\"\n",
    "print(\"Test 1 passed: The best feature index is an integer.\")\n",
    "\n",
    "# Test if the returned feature index is within the correct range\n",
    "assert 0 <= best_feature < len(data[0]) - 1, \"Test failed: Feature index should be within valid range\"\n",
    "print(\"Test 2 passed: The feature index is within valid range.\")\n",
    "\n",
    "# Test if the selected best feature is either Feature 1 (shape) or Feature 2 (size), based on the dataset used\n",
    "assert best_feature in [1, 2], \"Test failed: The selected best feature should be either Feature 1 or Feature 2\"\n",
    "print(\"Test 3 passed: The best feature is either Feature 1 or Feature 2.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train Documentation\n",
    "\n",
    "Recursively builds a decision tree using the ID3 algorithm based on entropy and information gain. The tree is represented as a nested dictionary.\n",
    "\n",
    "Parameters:\n",
    "- data (list): The dataset used to train the decision tree.\n",
    "- target_index (int, optional): The index of the target feature (default is 0).\n",
    "\n",
    "Returns:\n",
    "- dict or str: A dictionary representing the decision tree, where internal nodes represent feature splits, or a string representing a class label if the node is a leaf.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, target_index=0):\n",
    "    labels = set(row[target_index] for row in data)\n",
    "    \n",
    "    # Base cases: if all data have the same label or no other features to split on\n",
    "    if len(labels) == 1:\n",
    "        return list(labels)[0]\n",
    "    if len(data[0]) == 1:  # Only the target label remains\n",
    "        return None\n",
    "    \n",
    "    # Determine the best feature to split, excluding the target\n",
    "    best_feature = best_split(data, target_index)\n",
    "    tree = {best_feature: {}}\n",
    "    splits = split_data(data, best_feature)\n",
    "    \n",
    "    for feature_value, subset in splits.items():\n",
    "        subtree = train([row[:best_feature] + row[best_feature + 1:] for row in subset], target_index)\n",
    "        tree[best_feature][feature_value] = subtree\n",
    "        \n",
    "    return tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1 passed: Tree is trained successfully and is a dictionary.\n",
      "Test 2 passed: Tree contains a valid root feature that is not the class label.\n",
      "Test 3 passed: Leaf nodes are valid.\n"
     ]
    }
   ],
   "source": [
    "data = [['e', 'round'], ['p', 'square'], ['e', 'round'], ['p', 'square']]\n",
    "\n",
    "# Test if the tree is trained without errors\n",
    "tree = train(data)\n",
    "assert isinstance(tree, dict), \"Test failed: The tree should be a dictionary\"\n",
    "print(\"Test 1 passed: Tree is trained successfully and is a dictionary.\")\n",
    "\n",
    "# Test if the tree contains a valid feature as the root (any feature index within range)\n",
    "# Ensure the tree's root feature is a valid feature (not the target class label)\n",
    "root_feature = list(tree.keys())[0]\n",
    "assert 1 <= root_feature < len(data[0]), \"Test failed: The root should split on a valid feature, excluding the class label\"\n",
    "print(\"Test 2 passed: Tree contains a valid root feature that is not the class label.\")\n",
    "\n",
    "\n",
    "root_feature = list(tree.keys())[0]\n",
    "\n",
    "# Test if the tree contains valid leaf nodes (leaves should be strings)\n",
    "assert all(isinstance(val, str) for val in tree[root_feature].values()), \"Test failed: Leaf nodes should be strings\"\n",
    "print(\"Test 3 passed: Leaf nodes are valid.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classify Documentation\n",
    "\n",
    "Classifies a given observation using the decision tree generated by the `train` function.\n",
    "\n",
    "Parameters:\n",
    "- tree (dict): The decision tree used for classification.\n",
    "- observation (list): A single observation (data point) to classify.\n",
    "\n",
    "Returns:\n",
    "- str: The predicted class label for the observation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(tree, observation):\n",
    "    if not isinstance(tree, dict):\n",
    "        return tree  # Leaf node\n",
    "    feature = list(tree.keys())[0]\n",
    "    feature_value = observation[feature]\n",
    "    if feature_value in tree[feature]:\n",
    "        return classify(tree[feature][feature_value], observation)\n",
    "    else:\n",
    "        return None \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1 passed: Classification returns 'e' for round.\n",
      "Test 2 passed: Classification returns 'p' for square.\n",
      "Test 3 passed: Classification returns None for unknown category.\n"
     ]
    }
   ],
   "source": [
    "tree = {1: {'round': 'e', 'square': 'p'}}\n",
    "observation = ['e', 'round']\n",
    "\n",
    "# Test if the classifier returns the correct label\n",
    "assert classify(tree, observation) == 'e', \"Test failed: Classification should return 'e' for round\"\n",
    "print(\"Test 1 passed: Classification returns 'e' for round.\")\n",
    "\n",
    "# Test classification of a different observation\n",
    "observation_2 = ['p', 'square']\n",
    "assert classify(tree, observation_2) == 'p', \"Test failed: Classification should return 'p' for square\"\n",
    "print(\"Test 2 passed: Classification returns 'p' for square.\")\n",
    "\n",
    "# Test if the function returns None for missing branch\n",
    "observation_3 = ['p', 'triangle']\n",
    "assert classify(tree, observation_3) is None, \"Test failed: Classification should return None for unknown category\"\n",
    "print(\"Test 3 passed: Classification returns None for unknown category.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate Documentation\n",
    "\n",
    "Calculates the classification error rate, which is the proportion of misclassified instances.\n",
    "\n",
    "Parameters:\n",
    "- actual (list): The actual class labels.\n",
    "- predicted (list): The predicted class labels.\n",
    "\n",
    "Returns:\n",
    "- float: The error rate, calculated as the ratio of misclassified instances to the total number of instances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(actual, predicted):\n",
    "    errors = sum(1 for a, p in zip(actual, predicted) if a != p)\n",
    "    return errors / len(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1 passed: Error rate is calculated correctly.\n",
      "Test 2 passed: Error rate is 0 for perfect prediction.\n",
      "Test 3 passed: Error rate is a float.\n"
     ]
    }
   ],
   "source": [
    "actual = ['e', 'p', 'e', 'p']\n",
    "predicted = ['e', 'p', 'p', 'p']\n",
    "\n",
    "# Test if the error rate is calculated correctly\n",
    "assert evaluate(actual, predicted) == 0.25, \"Test failed: Error rate should be 0.25\"\n",
    "print(\"Test 1 passed: Error rate is calculated correctly.\")\n",
    "\n",
    "# Test if error rate is 0 for perfect prediction\n",
    "predicted_perfect = ['e', 'p', 'e', 'p']\n",
    "assert evaluate(actual, predicted_perfect) == 0.0, \"Test failed: Error rate should be 0 for perfect prediction\"\n",
    "print(\"Test 2 passed: Error rate is 0 for perfect prediction.\")\n",
    "\n",
    "# Test if the error rate is a float\n",
    "assert isinstance(evaluate(actual, predicted), float), \"Test failed: Error rate should be a float\"\n",
    "print(\"Test 3 passed: Error rate is a float.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pretty_print_tree Documentation\n",
    "\n",
    "Recursively prints the decision tree in a human-readable format, showing each feature and the corresponding split.\n",
    "\n",
    "Parameters:\n",
    "- tree (dict): The decision tree to be printed.\n",
    "- depth (int, optional): The depth of the current node, used for indentation (default is 0).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_tree(tree, depth=0):\n",
    "    if not isinstance(tree, dict):\n",
    "        print(\"  \" * depth + f\"Leaf: {tree}\")\n",
    "    else:\n",
    "        feature = list(tree.keys())[0]  # Get the feature being split on\n",
    "        for feature_value, subtree in tree[feature].items():\n",
    "            print(\"  \" * depth + f\"Feature {feature} = {feature_value}:\")  # Print the current node\n",
    "            pretty_print_tree(subtree, depth + 1)  # Recursively print subtrees\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 1 = round:\n",
      "  Leaf: e\n",
      "Feature 1 = square:\n",
      "  Leaf: p\n",
      "Test 1 passed: Tree printed successfully.\n",
      "Test 2 passed: Tree has correct branches.\n",
      "Test 3 passed: Tree leaves are strings.\n"
     ]
    }
   ],
   "source": [
    "tree = {1: {'round': 'e', 'square': 'p'}}\n",
    "\n",
    "# Test if tree is printed without errors\n",
    "try:\n",
    "    pretty_print_tree(tree)\n",
    "    print(\"Test 1 passed: Tree printed successfully.\")\n",
    "except Exception as e:\n",
    "    assert False, f\"Test failed: pretty_print_tree raised an exception: {e}\"\n",
    "\n",
    "# Test if the tree has correct depth\n",
    "assert len(tree[1]) == 2, \"Test failed: Tree should have two branches for 'round' and 'square'\"\n",
    "print(\"Test 2 passed: Tree has correct branches.\")\n",
    "\n",
    "# Test if the leaves are strings\n",
    "assert all(isinstance(val, str) for val in tree[1].values()), \"Test failed: Tree leaves should be strings\"\n",
    "print(\"Test 3 passed: Tree leaves are strings.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create_folds Documentation\n",
    "\n",
    "Splits the dataset into `n` random folds for cross-validation using `numpy`'s random permutation.\n",
    "\n",
    "Parameters:\n",
    "- xs (list): The dataset to be split into folds.\n",
    "- n (int): The number of folds to create.\n",
    "\n",
    "Returns:\n",
    "- list: A list of folds, where each fold is a subset of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folds(xs: list, n: int) -> list[list]:\n",
    "    fold_size = len(xs) // n\n",
    "    indices = np.random.permutation(len(xs))  # Generate random index array\n",
    "    folds = [xs[int(i * fold_size):int((i + 1) * fold_size)] for i in range(n)]\n",
    "    if len(xs) % n != 0:\n",
    "        remainder = xs[int(n * fold_size):]\n",
    "        folds[-1].extend(remainder)\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1 passed: Correct number of folds created.\n",
      "Test 2 passed: Data is evenly distributed in folds.\n",
      "Test 3 passed: Folds contain lists.\n"
     ]
    }
   ],
   "source": [
    "data = [['e'], ['p'], ['e'], ['p'], ['e']]\n",
    "\n",
    "# Test if the correct number of folds is created\n",
    "folds = create_folds(data, 2)\n",
    "assert len(folds) == 2, \"Test failed: There should be 2 folds\"\n",
    "print(\"Test 1 passed: Correct number of folds created.\")\n",
    "\n",
    "# Test if the data is evenly distributed in folds\n",
    "assert len(folds[0]) == 2 and len(folds[1]) == 3, \"Test failed: Data should be split into correct fold sizes\"\n",
    "print(\"Test 2 passed: Data is evenly distributed in folds.\")\n",
    "\n",
    "# Test if folds contain the expected data\n",
    "assert all(isinstance(fold, list) for fold in folds), \"Test failed: Folds should contain lists\"\n",
    "print(\"Test 3 passed: Folds contain lists.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create_train_test Documentation\n",
    "\n",
    "Combines folds into a training set and isolates one fold as the test set for cross-validation.\n",
    "\n",
    "Parameters:\n",
    "- folds (list): The list of folds generated from the dataset.\n",
    "- index (int): The index of the fold to be used as the test set.\n",
    "\n",
    "Returns:\n",
    "- tuple: A tuple containing the training set (list) and the test set (list).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test(folds: list[list], index: int) -> tuple[list, list]:\n",
    "    test = folds[index]\n",
    "    train = [item for i, fold in enumerate(folds) if i != index for item in fold]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1 passed: Train set has the correct number of rows.\n",
      "Test 2 passed: Test set has the correct number of rows.\n",
      "Test 3 passed: Train and test sets contain valid data.\n"
     ]
    }
   ],
   "source": [
    "folds = [[['e'], ['p']], [['e'], ['p'], ['e']]]\n",
    "\n",
    "# Test if train and test sets are created correctly\n",
    "training, test = create_train_test(folds, 1)\n",
    "\n",
    "# Test 1: Check if the training set has the correct number of rows\n",
    "assert len(training) == 2, \"Test failed: Train set should have 2 rows\"\n",
    "print(\"Test 1 passed: Train set has the correct number of rows.\")\n",
    "\n",
    "# Test 2: Check if the test set has the correct number of rows\n",
    "assert len(test) == 3, \"Test failed: Test set should have 3 rows\"\n",
    "print(\"Test 2 passed: Test set has the correct number of rows.\")\n",
    "\n",
    "# Test 3: Check if train and test sets contain valid data (lists)\n",
    "assert all(isinstance(row, list) for row in training + test), \"Test failed: Train and test sets should contain lists\"\n",
    "print(\"Test 3 passed: Train and test sets contain valid data.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cross_validate Documentation\n",
    "\n",
    "Performs k-fold cross-validation on the dataset using the decision tree classifier and returns the error rate for each fold.\n",
    "\n",
    "Parameters:\n",
    "- data (list): The dataset to be used for cross-validation.\n",
    "- folds (int, optional): The number of folds to use for cross-validation (default is 5).\n",
    "- target_index (int, optional): The index of the target feature (default is 0).\n",
    "\n",
    "Returns:\n",
    "- list: A list of error rates for each fold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(data, folds=5, target_index=0):\n",
    "    fold_data = create_folds(data, folds)\n",
    "    error_rates = []\n",
    "    \n",
    "    for i in range(folds):\n",
    "        train_data, test_data = create_train_test(fold_data, i)\n",
    "        tree = train(train_data, target_index)\n",
    "        actual = [row[target_index] for row in test_data]\n",
    "        predictions = [classify(tree, row) for row in test_data]\n",
    "        error_rate = evaluate(actual, predictions)\n",
    "        error_rates.append(error_rate)\n",
    "        print(f\"Fold {i + 1}, Error rate: {error_rate:.4f}\")\n",
    "\n",
    "    return error_rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Error rate: 0.0000\n",
      "Fold 2, Error rate: 0.0000\n",
      "Test 1 passed: cross_validate returns a list of error rates.\n",
      "Test 2 passed: The number of error rates matches the number of folds.\n",
      "Test 3 passed: Each error rate is a float.\n"
     ]
    }
   ],
   "source": [
    "# Run cross-validation on a sample dataset\n",
    "data = [['e', 'round'], ['p', 'square'], ['e', 'round'], ['p', 'square']]\n",
    "\n",
    "# Test 1: Check if cross-validation returns a list of error rates\n",
    "error_rates = cross_validate(data, folds=2)\n",
    "assert isinstance(error_rates, list), \"Test failed: cross_validate should return a list of error rates\"\n",
    "print(\"Test 1 passed: cross_validate returns a list of error rates.\")\n",
    "\n",
    "# Test 2: Check if the number of error rates matches the number of folds\n",
    "assert len(error_rates) == 2, \"Test failed: The number of error rates should match the number of folds\"\n",
    "print(\"Test 2 passed: The number of error rates matches the number of folds.\")\n",
    "\n",
    "# Test 3: Check if each error rate is a float\n",
    "assert all(isinstance(rate, float) for rate in error_rates), \"Test failed: Each error rate should be a float\"\n",
    "print(\"Test 3 passed: Each error rate is a float.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Error rate: 0.4973\n",
      "Fold 2, Error rate: 0.4982\n",
      "Fold 3, Error rate: 0.4973\n",
      "Fold 4, Error rate: 0.4938\n",
      "Fold 5, Error rate: 0.4726\n",
      "Feature 5 = a:\n",
      "  Leaf: e\n",
      "Feature 5 = c:\n",
      "  Leaf: p\n",
      "Feature 5 = f:\n",
      "  Leaf: p\n",
      "Feature 5 = l:\n",
      "  Leaf: e\n",
      "Feature 5 = m:\n",
      "  Leaf: p\n",
      "Feature 5 = n:\n",
      "  Feature 19 = k:\n",
      "    Leaf: e\n",
      "  Feature 19 = n:\n",
      "    Leaf: e\n",
      "  Feature 19 = r:\n",
      "    Leaf: p\n",
      "  Feature 19 = w:\n",
      "    Feature 3 = c:\n",
      "      Leaf: e\n",
      "    Feature 3 = g:\n",
      "      Leaf: e\n",
      "    Feature 3 = n:\n",
      "      Leaf: e\n",
      "    Feature 3 = p:\n",
      "      Leaf: e\n",
      "    Feature 3 = w:\n",
      "      Leaf: p\n",
      "    Feature 3 = y:\n",
      "      Leaf: p\n",
      "Feature 5 = p:\n",
      "  Leaf: p\n"
     ]
    }
   ],
   "source": [
    "cross_validate(cleaned_data, folds=5)\n",
    "\n",
    "final_tree = train(cleaned_data)\n",
    "pretty_print_tree(final_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I fixed the problem of including the class label as a feature, but my model has low predictive power and I don't know why. It's almost as likely to be incorrect as correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before You Submit...\n",
    "\n",
    "1. Did you provide output exactly as requested?\n",
    "2. Did you re-execute the entire notebook? (\"Restart Kernel and Rull All Cells...\")\n",
    "3. If you did not complete the assignment or had difficulty please explain what gave you the most difficulty in the Markdown cell below.\n",
    "4. Did you change the name of the file to `jhed_id.ipynb`?\n",
    "\n",
    "Do not submit any other files."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (en605645)",
   "language": "python",
   "name": "en605645"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "81px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

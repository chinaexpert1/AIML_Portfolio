{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 9 - Programming Assignment\n",
    "\n",
    "## Directions\n",
    "\n",
    "1. Change the name of this file to be your JHED id as in `jsmith299.ipynb`. Because sure you use your JHED ID (it's made out of your name and not your student id which is just letters and numbers).\n",
    "2. Make sure the notebook you submit is cleanly and fully executed. I do not grade unexecuted notebooks.\n",
    "3. Submit your notebook back in Blackboard where you downloaded this file.\n",
    "\n",
    "*Provide the output **exactly** as requested*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier\n",
    "\n",
    "For this assignment you will be implementing and evaluating a Naive Bayes Classifier with the same data from last week:\n",
    "\n",
    "http://archive.ics.uci.edu/ml/datasets/Mushroom\n",
    "\n",
    "(You should have downloaded it).\n",
    "\n",
    "<div style=\"background: lemonchiffon; margin:20px; padding: 20px;\">\n",
    "    <strong>Important</strong>\n",
    "    <p>\n",
    "        No Pandas. The only acceptable libraries in this class are those contained in the `environment.yml`. No OOP, either. You can used Dicts, NamedTuples, etc. as your abstract data type (ADT) for the the tree and nodes.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "\n",
    "You'll first need to calculate all of the necessary probabilities using a `train` function. A flag will control whether or not you use \"+1 Smoothing\" or not. You'll then need to have a `classify` function that takes your probabilities, a List of instances (possibly a list of 1) and returns a List of Tuples. Each Tuple has the best class in the first position and a dict with a key for every possible class label and the associated *normalized* probability. For example, if we have given the `classify` function a list of 2 observations, we would get the following back:\n",
    "\n",
    "```\n",
    "[(\"e\", {\"e\": 0.98, \"p\": 0.02}), (\"p\", {\"e\": 0.34, \"p\": 0.66})]\n",
    "```\n",
    "\n",
    "when calculating the error rate of your classifier, you should pick the class label with the highest probability; you can write a simple function that takes the Dict and returns that class label.\n",
    "\n",
    "As a reminder, the Naive Bayes Classifier generates the *unnormalized* probabilities from the numerator of Bayes Rule:\n",
    "\n",
    "$$P(C|A) \\propto P(A|C)P(C)$$\n",
    "\n",
    "where C is the class and A are the attributes (data). Since the normalizer of Bayes Rule is the *sum* of all possible numerators and you have to calculate them all, the normalizer is just the sum of the probabilities.\n",
    "\n",
    "You will have the same basic functions as the last module's assignment and some of them can be reused or at least repurposed.\n",
    "\n",
    "`train` takes training_data and returns a Naive Bayes Classifier (NBC) as a data structure. There are many options including namedtuples and just plain old nested dictionaries. **No OOP**.\n",
    "\n",
    "```\n",
    "def train(training_data, smoothing=True):\n",
    "   # returns the Decision Tree.\n",
    "```\n",
    "\n",
    "The `smoothing` value defaults to True. You should handle both cases.\n",
    "\n",
    "`classify` takes a NBC produced from the function above and applies it to labeled data (like the test set) or unlabeled data (like some new data). (This is not the same `classify` as the pseudocode which classifies only one instance at a time; it can call it though).\n",
    "\n",
    "```\n",
    "def classify(nbc, observations, labeled=True):\n",
    "    # returns a list of tuples, the argmax and the raw data as per the pseudocode.\n",
    "```\n",
    "\n",
    "`evaluate` takes a data set with labels (like the training set or test set) and the classification result and calculates the classification error rate:\n",
    "\n",
    "$$error\\_rate=\\frac{errors}{n}$$\n",
    "\n",
    "Do not use anything else as evaluation metric or the submission will be deemed incomplete, ie, an \"F\". (Hint: accuracy rate is not the error rate!).\n",
    "\n",
    "`cross_validate` takes the data and uses 10 fold cross validation (from Module 3!) to `train`, `classify`, and `evaluate`. **Remember to shuffle your data before you create your folds**. I leave the exact signature of `cross_validate` to you but you should write it so that you can use it with *any* `classify` function of the same form (using higher order functions and partial application). If you did so last time, you can reuse it for this assignment.\n",
    "\n",
    "Following Module 3's discussion, `cross_validate` should print out the fold number and the evaluation metric (error rate) for each fold and then the average value (and the variance). What you are looking for here is a consistent evaluation metric cross the folds. You should print the error rates in terms of percents (ie, multiply the error rate by 100 and add \"%\" to the end).\n",
    "\n",
    "To summarize...\n",
    "\n",
    "Apply the Naive Bayes Classifier algorithm to the Mushroom data set using 10 fold cross validation and the error rate as the evaluation metric. You will do this *twice*. Once with smoothing=True and once with smoothing=False. You should follow up with a brief explanation for the similarities or differences in the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load_data documentation\n",
    "\n",
    "load_data(file_path: str) -> List[List[str]]  \n",
    "    Reads a CSV file containing the mushroom data, dropping rows with any missing data, and returns a list of rows where each row is a list of feature values.  \n",
    "\n",
    "    Parameters:  \n",
    "    - file_path (str): The path to the CSV file.  \n",
    "\n",
    "    Returns:\n",
    "    - List[List[str]]: A list of lists, each inner list representing a row of feature values.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "def load_data(file_path: str) -> List[List[str]]:\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            if \"?\" not in row:  \n",
    "                data.append(row)\n",
    "    return data\n",
    "\n",
    "file_path = 'agaricus-lepiota.data'\n",
    "data = load_data(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_data test 1 passed!\n",
      "load_data test 2 passed!\n",
      "load_data test 3 passed!\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Data should be a list of lists\n",
    "data = load_data(file_path)\n",
    "assert isinstance(data, list), \"Data should be a list\"\n",
    "assert all(isinstance(row, list) for row in data), \"Each row should be a list\"\n",
    "print(\"load_data test 1 passed!\")\n",
    "\n",
    "# Test 2: Data should contain no missing values\n",
    "assert all(\"?\" not in row for row in data), \"Data should not contain missing values\"\n",
    "print(\"load_data test 2 passed!\")\n",
    "\n",
    "# Test 3: Data should not be empty\n",
    "assert len(data) > 0, \"Data should not be empty\"\n",
    "print(\"load_data test 3 passed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE THIS CELL\n",
    "# IGNORE THIS CELL\n",
    "# FUNCTION REPEATED BELOW\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from typing import NamedTuple, Callable\n",
    "import numpy as np\n",
    "\n",
    "class NaiveBayesClassifier(NamedTuple):\n",
    "    class_priors: Dict[str, float]\n",
    "    conditional_probs: Dict[str, Dict[int, Dict[str, float]]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train documentation  \n",
    "\n",
    "train(data: List[List[str]], smoothing: bool = True) -> NaiveBayesClassifier  \n",
    "    Trains a Naive Bayes Classifier on the given data, calculating prior and conditional probabilities for each class and feature value. Supports optional Laplace smoothing.  \n",
    "\n",
    "    Parameters:  \n",
    "    - data (List[List[str]]): The dataset with each row as a list of feature values. The first element in each row is the class label.  \n",
    "    - smoothing (bool): If True, applies Laplace smoothing to conditional probabilities.  \n",
    "\n",
    "    Returns:  \n",
    "    - NaiveBayesClassifier: A named tuple containing class priors and conditional probabilities.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(data: List[List[str]], smoothing: bool = True):\n",
    "    class_counts = Counter(row[0] for row in data)\n",
    "    total_count = len(data)\n",
    "    \n",
    "    class_priors = {cls: count / total_count for cls, count in class_counts.items()}\n",
    "    \n",
    "    conditional_probs = {cls: defaultdict(lambda: defaultdict(float)) for cls in class_counts}\n",
    "    \n",
    "    for cls in class_counts:\n",
    "        # Filter data by class\n",
    "        class_data = np.array([row for row in data if row[0] == cls])\n",
    "        \n",
    "        for feature_index in range(1, len(data[0])):\n",
    "            feature_values, counts = np.unique(class_data[:, feature_index], return_counts=True)\n",
    "            total_feature_count = counts.sum()\n",
    "            unique_values = len(feature_values)  # Number of unique feature values\n",
    "            \n",
    "            for value, count in zip(feature_values, counts):\n",
    "                if smoothing:\n",
    "                    conditional_probs[cls][feature_index][value] = (count + 1) / (total_feature_count + unique_values)\n",
    "                else:\n",
    "                    conditional_probs[cls][feature_index][value] = count / total_feature_count\n",
    "\n",
    "    return NaiveBayesClassifier(class_priors, conditional_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train test 1 passed!\n",
      "train test 2 passed!\n",
      "train test 3 passed!\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Check if output is NaiveBayesClassifier\n",
    "nbc = train(data, smoothing=True)\n",
    "assert isinstance(nbc, NaiveBayesClassifier), \"Output should be a NaiveBayesClassifier\"\n",
    "print(\"train test 1 passed!\")\n",
    "\n",
    "# Test 2: Class priors should include keys for \"e\" and \"p\"\n",
    "assert \"e\" in nbc.class_priors and \"p\" in nbc.class_priors, \"Class priors should include 'e' and 'p'\"\n",
    "print(\"train test 2 passed!\")\n",
    "\n",
    "# Test 3: Conditional probabilities should be dictionaries of dictionaries\n",
    "assert isinstance(nbc.conditional_probs[\"e\"], dict) and isinstance(nbc.conditional_probs[\"p\"], dict), \"Conditional probabilities should be a dictionary of dictionaries\"\n",
    "print(\"train test 3 passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NaiveBayesClassifier documentation  \n",
    "\n",
    "NaiveBayesClassifier  \n",
    "    A named tuple that stores the trained Naive Bayes Classifier, containing class prior probabilities and conditional probabilities for each feature and class.  \n",
    "\n",
    "    Attributes:  \n",
    "    - class_priors (Dict[str, float]): A dictionary with classes as keys and their respective prior probabilities as values.  \n",
    "    - conditional_probs (Dict[str, Dict[int, Dict[str, float]]]): A nested dictionary structure containing conditional probabilities.  \n",
    "        - Outer Dict[str]: Keys are class labels (e.g., \"e\" or \"p\" for edible or poisonous).  \n",
    "        - Inner Dict[int]: Keys are feature indices (1-indexed) corresponding to each feature.  \n",
    "        - Innermost Dict[str, float]: Keys are feature values with their respective conditional probabilities given the class.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from typing import NamedTuple, Callable\n",
    "import numpy as np\n",
    "\n",
    "class NaiveBayesClassifier(NamedTuple):\n",
    "    class_priors: Dict[str, float]\n",
    "    conditional_probs: Dict[str, Dict[int, Dict[str, float]]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaiveBayesClassifier test 1 passed!\n",
      "NaiveBayesClassifier test 2 passed!\n",
      "NaiveBayesClassifier test 3 passed!\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Check if NaiveBayesClassifier has required attributes\n",
    "nbc = train(data, smoothing=True)\n",
    "assert hasattr(nbc, \"class_priors\"), \"NaiveBayesClassifier shoulld have a 'class_priors' attribute\"\n",
    "assert hasattr(nbc, \"conditional_probs\"), \"NaiveBayesClassifier should have a 'conditional_probs' attribute\"\n",
    "print(\"NaiveBayesClassifier test 1 passed!\")\n",
    "\n",
    "# Test 2: class_priors should contain probabilities for each class (\"e\" and \"p\")\n",
    "assert \"e\" in nbc.class_priors and \"p\" in nbc.class_priors, \"class_priors should include 'e' and 'p'\"\n",
    "assert 0 <= nbc.class_priors[\"e\"] <= 1, \"'e' class prior should be between 0 and 1\"\n",
    "assert 0 <= nbc.class_priors[\"p\"] <= 1, \"'p' class prior should be between 0 and 1\"\n",
    "print(\"NaiveBayesClassifier test 2 passed!\")\n",
    "\n",
    "# Test 3: conditional_probs should have feature-wise probabilities\n",
    "sample_feature_index = 1  # check first feature\n",
    "sample_class = \"e\"\n",
    "assert isinstance(nbc.conditional_probs[sample_class][sample_feature_index], dict), \"conditional_probs should contain dictioaries of probabilities\"\n",
    "assert all(0 <= prob <= 1 for prob in nbc.conditional_probs[sample_class][sample_feature_index].values()), \"All conditional probabilities should be between 0 and 1\"\n",
    "print(\"NaiveBayesClassifier test 3 passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classify documentation  \n",
    "\n",
    "classify(nbc: NaiveBayesClassifier, observations: List[List[str]], labeled: bool = True) -> List[Tuple[str, Dict[str, float]]]  \n",
    "    Classifies a list of observations using the trained Naive Bayes Classifier, calculating and normalizing probabilities for each class.  \n",
    "\n",
    "    Parameters:  \n",
    "    - nbc (NaiveBayesClassifier): A trained Naive Bayes Classifier with calculated priors and conditional probabilities.  \n",
    "    - observations (List[List[str]]): A list of observations to classify, where each observation is a list of feature values.  \n",
    "    - labeled (bool): If True, indicates that each observation contains a label at the first index, which will be skipped in classification.  \n",
    "\n",
    "    Returns:  \n",
    "    - List[Tuple[str, Dict[str, float]]]: A list of tuples, each containing:  \n",
    "        - The class with the highest probability.  \n",
    "        - A dictionary of normalized probabilities for each class.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def classify(nbc: NaiveBayesClassifier, observations: List[List[str]], labeled: bool = True) -> List[Tuple[str, Dict[str, float]]]:\n",
    "    results = []\n",
    "    for observation in observations:\n",
    "        if labeled:\n",
    "            observation = observation[1:]  \n",
    "\n",
    "        class_probs = {}\n",
    "        for cls in nbc.class_priors:\n",
    "            prob = nbc.class_priors[cls]  \n",
    "            \n",
    "            for feature_index, feature_value in enumerate(observation, start=1):\n",
    "                prob *= nbc.conditional_probs[cls][feature_index].get(feature_value, 1e-6)\n",
    "            class_probs[cls] = prob\n",
    "\n",
    "        total_prob = np.sum(list(class_probs.values()))\n",
    "        normalized_probs = {cls: prob / total_prob for cls, prob in class_probs.items()}\n",
    "        \n",
    "        best_class = max(normalized_probs, key=normalized_probs.get)\n",
    "        results.append((best_class, normalized_probs))\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classify test 1 passed!\n",
      "classify test 2 passed!\n",
      "classify test 3 passed!\n"
     ]
    }
   ],
   "source": [
    "# Test 1: classify should return a list of tuples\n",
    "sample_observations = data[:5]\n",
    "classify_results = classify(nbc, sample_observations, labeled=True)\n",
    "assert isinstance(classify_results, list), \"Classsify result should be a list\"\n",
    "assert all(isinstance(result, tuple) for result in classify_results), \"Each result should be a tuple\"\n",
    "print(\"classify test 1 passed!\")\n",
    "\n",
    "# Test 2: Each tuple should contain a best class (str) and probability dict\n",
    "assert all(isinstance(result[0], str) for result in classify_results), \"Each best class should be a string\"\n",
    "assert all(isinstance(result[1], dict) for result in classify_results), \"Each probability should be a dict\"\n",
    "print(\"classify test 2 passed!\")\n",
    "\n",
    "# Test 3: Probability dictionary should sum to 1 (or close due to rounding)\n",
    "for _, prob_dict in classify_results:\n",
    "    assert np.isclose(sum(prob_dict.values()), 1.0), \"Probabilities should sum to 1\"\n",
    "print(\"classify test 3 passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate documentation  \n",
    "\n",
    "evaluate(predictions: List[Tuple[str, Dict[str, float]]], true_labels: List[str]) -> float  \n",
    "    Computes the error rate by comparing predicted labels with true labels in a labeled dataset.  \n",
    "\n",
    "    Parameters:  \n",
    "    - predictions (List[Tuple[str, Dict[str, float]]]): A list of tuples containing the predicted class and class probabilities for each observation.  \n",
    "    - true_labels (List[str]): The true class labels for each observation.  \n",
    " \n",
    "    Returns:  \n",
    "    - float: The classification error rate as a percentage.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(predictions: List[Tuple[str, Dict[str, float]]], true_labels: List[str]) -> float:\n",
    "    errors = np.sum([pred[0] != true for pred, true in zip(predictions, true_labels)])\n",
    "    error_rate = errors / len(true_labels)\n",
    "    return error_rate * 100  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate test 1 passed!\n",
      "evaluate test 2 passed!\n",
      "evaluate test 3 passed!\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Evaluate should return a float\n",
    "true_labels = [row[0] for row in sample_observations]\n",
    "error_rate = evaluate(classify_results, true_labels)\n",
    "assert isinstance(error_rate, float), \"Error rate should be a float\"\n",
    "print(\"evaluate test 1 passed!\")\n",
    "\n",
    "# Test 2: Error rate should be between 0 and 100\n",
    "assert 0 <= error_rate <= 100, \"Error rate should be between 0 and 100\"\n",
    "print(\"evaluate test 2 passed!\")\n",
    "\n",
    "# Test 3: perfect classification should yield 0% error rate\n",
    "perfect_predictions = [(label, {\"e\": 1.0 if label == \"e\" else 0.0, \"p\": 1.0 if label == \"p\" else 0.0}) for label in true_labels]\n",
    "assert evaluate(perfect_predictions, true_labels) == 0, \"Perfect classification should have 0% error rate\"\n",
    "print(\"evaluate test 3 passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cross_validate documentation  \n",
    "\n",
    "cross_validate(data: List[List[str]], folds: int = 10, smoothing: bool = True) -> None  \n",
    "    Performs 10-fold cross-validation on the dataset, training and testing the Naive Bayes Classifier on each fold. Prints the error rate for each fold, the average error rate, and variance.  \n",
    "\n",
    "    Parameters:  \n",
    "    - data (List[List[str]]): The dataset with each row as a list of feature values. The first element in each row is the class label.  \n",
    "    - folds (int): The number of cross-validation folds (default is 10).  \n",
    "    - smoothing (bool): If True, applies Laplace smoothing to conditional probabilities during training.  \n",
    "\n",
    "    Returns:  \n",
    "    - None  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cross_validate(data: List[List[str]], folds: int = 10, smoothing: bool = True) -> None:\n",
    "    data_np = np.array(data)\n",
    "    np.random.shuffle(data_np)\n",
    "    fold_size = len(data_np) // folds\n",
    "    error_rates = []\n",
    "    \n",
    "    for fold in range(folds):\n",
    "        test_data = data_np[fold * fold_size : (fold + 1) * fold_size]\n",
    "        train_data = np.concatenate((data_np[:fold * fold_size], data_np[(fold + 1) * fold_size:]))\n",
    "        \n",
    "        nbc = train(train_data.tolist(), smoothing=smoothing)\n",
    "        \n",
    "        true_labels = test_data[:, 0]\n",
    "        predictions = classify(nbc, test_data.tolist(), labeled=True)\n",
    "        \n",
    "        error_rate = evaluate(predictions, true_labels.tolist())\n",
    "        error_rates.append(error_rate)\n",
    "        \n",
    "        print(f\"Fold {fold + 1}: Error Rate = {error_rate:.2f}%\")\n",
    "    \n",
    "    avg_error_rate = np.mean(error_rates)\n",
    "    error_variance = np.var(error_rates)\n",
    "    print(f\"\\nAverage Error Rate: {avg_error_rate:.2f}%\")\n",
    "    print(f\"Variance in Error Rate: {error_variance:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_validate test 1: Cross-validation execution check\n",
      "Fold 1: Error Rate = 0.53%\n",
      "Fold 2: Error Rate = 0.00%\n",
      "Fold 3: Error Rate = 0.35%\n",
      "Fold 4: Error Rate = 0.00%\n",
      "Fold 5: Error Rate = 0.18%\n",
      "Fold 6: Error Rate = 0.00%\n",
      "Fold 7: Error Rate = 0.53%\n",
      "Fold 8: Error Rate = 1.06%\n",
      "Fold 9: Error Rate = 0.00%\n",
      "Fold 10: Error Rate = 0.18%\n",
      "\n",
      "Average Error Rate: 0.28%\n",
      "Variance in Error Rate: 0.1081\n",
      "cross_validate test 1 passed!\n",
      "cross_validate test 2 passed!\n",
      "cross_validate test 3 passed!\n"
     ]
    }
   ],
   "source": [
    "# Test 1: CCross-validation runs without error \n",
    "print(\"cross_validate test 1: Cross-validation execution check\")\n",
    "cross_validate(data, folds=10, smoothing=True)\n",
    "print(\"cross_validate test 1 passed!\")\n",
    "\n",
    "# Test 2: Average error rate should be between 0 and 100\n",
    "error_rates = []\n",
    "for fold in range(10):\n",
    "    test_data = data[fold * (len(data) // 10): (fold + 1) * (len(data) // 10)]\n",
    "    train_data = data[:fold * (len(data) // 10)] + data[(fold + 1) * (len(data) // 10):]\n",
    "    nbc = train(train_data, smoothing=True)\n",
    "    predictions = classify(nbc, test_data, labeled=True)\n",
    "    true_labels = [row[0] for row in test_data]\n",
    "    error_rates.append(evaluate(predictions, true_labels))\n",
    "\n",
    "average_error_rate = np.mean(error_rates)\n",
    "assert 0 <= average_error_rate <= 100, \"Average error rate should be between 0 and 100\"\n",
    "print(\"cross_validate test 2 passed!\")\n",
    "\n",
    "# Test 3: Variance of error rates should be non-negative\n",
    "variance_error_rate = np.var(error_rates)\n",
    "assert variance_error_rate >= 0, \"Variance should be non-negative\"\n",
    "print(\"cross_validate test 3 passed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation with Smoothing:\n",
      "Fold 1: Error Rate = 0.53%\n",
      "Fold 2: Error Rate = 0.18%\n",
      "Fold 3: Error Rate = 0.00%\n",
      "Fold 4: Error Rate = 0.53%\n",
      "Fold 5: Error Rate = 0.00%\n",
      "Fold 6: Error Rate = 0.18%\n",
      "Fold 7: Error Rate = 0.18%\n",
      "Fold 8: Error Rate = 0.18%\n",
      "Fold 9: Error Rate = 0.71%\n",
      "Fold 10: Error Rate = 0.35%\n",
      "\n",
      "Average Error Rate: 0.28%\n",
      "Variance in Error Rate: 0.0516\n"
     ]
    }
   ],
   "source": [
    "# Perform cross-validation with smoothing=True \n",
    "print(\"Cross-Validation with Smoothing:\")\n",
    "cross_validate(data, smoothing=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-Validation without Smoothing:\n",
      "Fold 1: Error Rate = 0.53%\n",
      "Fold 2: Error Rate = 0.00%\n",
      "Fold 3: Error Rate = 0.53%\n",
      "Fold 4: Error Rate = 0.00%\n",
      "Fold 5: Error Rate = 0.35%\n",
      "Fold 6: Error Rate = 0.00%\n",
      "Fold 7: Error Rate = 0.18%\n",
      "Fold 8: Error Rate = 0.53%\n",
      "Fold 9: Error Rate = 0.35%\n",
      "Fold 10: Error Rate = 0.18%\n",
      "\n",
      "Average Error Rate: 0.27%\n",
      "Variance in Error Rate: 0.0456\n"
     ]
    }
   ],
   "source": [
    "# Perform cross-validation with smoothing=False\n",
    "print(\"\\nCross-Validation without Smoothing:\")\n",
    "cross_validate(data, smoothing=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With smoothing, the model demonstrates slightly higher consistency across different folds, as indicated by a broader spread of error rates and a slightly higher variance. Smoothing tends to stabilize the model by avoiding zero probabilities for rare feature occurrences, allowing it to generalize better on diverse subsets. So, this approach leads to some variability in the error rates across folds, as the model balances predictions for less common features.\n",
    "\n",
    "Without smoothing, the model’s error rates appear more clustered, indicating less fluctuation in performance across folds. However, the absence of smoothing means the model might predict certain observations incorrectly if it encounters feature combinations it hasn’t seen in training, especially in smaller folds. The result is a a more uniform but slightly less robust model as seen by the slightly lower variance but similar average performance.\n",
    "\n",
    "All in all, smoothing provides a more adaptable model withh slight trade-offs in variability, while the model without smoothing offers stable but less adaptable error rates across folds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before You Submit...\n",
    "\n",
    "1. Did you provide output exactly as requested?\n",
    "2. Did you re-execute the entire notebook? (\"Restart Kernel and Rull All Cells...\")\n",
    "3. If you did not complete the assignment or had difficulty please explain what gave you the most difficulty in the Markdown cell below.\n",
    "4. Did you change the name of the file to `jhed_id.ipynb`?\n",
    "\n",
    "Do not submit any other files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (en605645)",
   "language": "python",
   "name": "en605645"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "81px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# include your import statements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Task 5: Assignment Instructions:**\n",
    "A. Compare and contrast the performance of YOLO Model 1 and YOLO Model 2 by demonstrating the weaknesses and strengths of each model. \n",
    "B. Finally, select the best model and state the reasons why this model is best suited for the TechTrack's system implementation.\n",
    "\n",
    "**Reminder:**  \n",
    "- Your notebook should be well-structured and clear for effective presentation. Up to 10 points may be deducted for poor structure and clarity.\n",
    "- Consider this report as if it were being reviewed by **TechTrack stakeholders**. Keep it professional, insightful, and visually organized!\n",
    "- Use visualizations, tables, and quantitative analysis where applicable to support your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Task A:**\n",
    "**Compare and contrast the performance of YOLO Model 1 and YOLO Model 2 by demonstrating the weaknesses and strengths of each model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Model 1 Evaluation ------\n",
      "Class 0 (barcode): AP = 0.485, Max F1 = 0.632, Avg IoU = 0.281\n",
      "Class 1 (car): AP = 0.515, Max F1 = 0.618, Avg IoU = 0.376\n",
      "Class 2 (cardboard box): AP = 0.336, Max F1 = 0.497, Avg IoU = 0.292\n",
      "Class 3 (fire): AP = 0.019, Max F1 = 0.038, Avg IoU = 0.224\n",
      "Class 4 (forklift): AP = 0.385, Max F1 = 0.601, Avg IoU = 0.325\n",
      "Class 5 (freight container): AP = 0.152, Max F1 = 0.289, Avg IoU = 0.312\n",
      "Class 6 (gloves): AP = 0.450, Max F1 = 0.650, Avg IoU = 0.247\n",
      "Class 7 (helmet): AP = 0.111, Max F1 = 0.156, Avg IoU = 0.202\n",
      "Class 8 (ladder): AP = 0.168, Max F1 = 0.283, Avg IoU = 0.322\n",
      "Class 9 (license plate): AP = 0.091, Max F1 = 0.150, Avg IoU = 0.065\n",
      "Class 10 (person): AP = 0.200, Max F1 = 0.347, Avg IoU = 0.272\n",
      "Class 11 (qr code): AP = 0.438, Max F1 = 0.566, Avg IoU = 0.280\n",
      "Class 12 (road sign): AP = 0.091, Max F1 = 0.166, Avg IoU = 0.326\n",
      "Class 13 (safety vest): AP = 0.260, Max F1 = 0.402, Avg IoU = 0.258\n",
      "Class 14 (smoke): AP = 0.239, Max F1 = 0.344, Avg IoU = 0.272\n",
      "Class 15 (traffic cone): AP = 0.408, Max F1 = 0.531, Avg IoU = 0.316\n",
      "Class 16 (traffic light): AP = 0.239, Max F1 = 0.359, Avg IoU = 0.271\n",
      "Class 17 (truck): AP = 0.574, Max F1 = 0.756, Avg IoU = 0.392\n",
      "Class 18 (van): AP = 0.696, Max F1 = 0.781, Avg IoU = 0.387\n",
      "Class 19 (wood pallet): AP = 0.114, Max F1 = 0.163, Avg IoU = 0.223\n",
      "\n",
      "--- Overall Metrics for Model 1 ---\n",
      "Model 1 mAP: 0.299\n",
      "Model 1 mF1: 0.416\n",
      "Model 1 mAvgIoU: 0.282\n",
      "\n",
      "------ Model 2 Evaluation ------\n",
      "Class 0 (barcode): AP = 0.546, Max F1 = 0.694, Avg IoU = 0.272\n",
      "Class 1 (car): AP = 0.511, Max F1 = 0.627, Avg IoU = 0.379\n",
      "Class 2 (cardboard box): AP = 0.351, Max F1 = 0.506, Avg IoU = 0.291\n",
      "Class 3 (fire): AP = 0.029, Max F1 = 0.081, Avg IoU = 0.228\n",
      "Class 4 (forklift): AP = 0.541, Max F1 = 0.693, Avg IoU = 0.359\n",
      "Class 5 (freight container): AP = 0.218, Max F1 = 0.324, Avg IoU = 0.310\n",
      "Class 6 (gloves): AP = 0.541, Max F1 = 0.701, Avg IoU = 0.264\n",
      "Class 7 (helmet): AP = 0.116, Max F1 = 0.189, Avg IoU = 0.200\n",
      "Class 8 (ladder): AP = 0.237, Max F1 = 0.416, Avg IoU = 0.312\n",
      "Class 9 (license plate): AP = 0.148, Max F1 = 0.189, Avg IoU = 0.070\n",
      "Class 10 (person): AP = 0.253, Max F1 = 0.392, Avg IoU = 0.279\n",
      "Class 11 (qr code): AP = 0.435, Max F1 = 0.583, Avg IoU = 0.294\n",
      "Class 12 (road sign): AP = 0.134, Max F1 = 0.182, Avg IoU = 0.323\n",
      "Class 13 (safety vest): AP = 0.278, Max F1 = 0.446, Avg IoU = 0.245\n",
      "Class 14 (smoke): AP = 0.245, Max F1 = 0.412, Avg IoU = 0.286\n",
      "Class 15 (traffic cone): AP = 0.401, Max F1 = 0.548, Avg IoU = 0.337\n",
      "Class 16 (traffic light): AP = 0.279, Max F1 = 0.394, Avg IoU = 0.282\n",
      "Class 17 (truck): AP = 0.662, Max F1 = 0.801, Avg IoU = 0.414\n",
      "Class 18 (van): AP = 0.706, Max F1 = 0.799, Avg IoU = 0.412\n",
      "Class 19 (wood pallet): AP = 0.118, Max F1 = 0.163, Avg IoU = 0.242\n",
      "\n",
      "--- Overall Metrics for Model 2 ---\n",
      "Model 2 mAP: 0.337\n",
      "Model 2 mF1: 0.457\n",
      "Model 2 mAvgIoU: 0.290\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def load_annotations(file_path):\n",
    "    \"\"\"\n",
    "    Loads bounding boxes and class IDs from a file.\n",
    "    Expected format per line: class_id x y width height [confidence (optional)]\n",
    "    The class_id is expected to be an integer but may be stored as a float string.\n",
    "    \"\"\"\n",
    "    boxes = []\n",
    "    classes = []\n",
    "    confidences = []\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 5:\n",
    "                    cls, x, y, w, h = parts[:5]\n",
    "                    boxes.append([float(x), float(y), float(w), float(h)])\n",
    "                    # Convert class to int (even if stored as a float string)\n",
    "                    classes.append(int(float(cls)))\n",
    "                    if len(parts) == 6:\n",
    "                        confidences.append(float(parts[5]))\n",
    "                    else:\n",
    "                        confidences.append(1.0)\n",
    "    return boxes, classes, confidences\n",
    "\n",
    "\n",
    "def compute_iou(boxA, boxB):\n",
    "    \"\"\"\n",
    "    Computes the Intersection over Union (IoU) of two bounding boxes.\n",
    "    Boxes are in the format [x, y, width, height].\n",
    "    \"\"\"\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[0] + boxA[2], boxB[0] + boxB[2])\n",
    "    yB = min(boxA[1] + boxA[3], boxB[1] + boxB[3])\n",
    "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "    boxAArea = boxA[2] * boxA[3]\n",
    "    boxBArea = boxB[2] * boxB[3]\n",
    "    unionArea = boxAArea + boxBArea - interArea\n",
    "    return interArea / unionArea if unionArea != 0 else 0\n",
    "\n",
    "\n",
    "def compute_ap(recalls, precisions):\n",
    "    \"\"\"\n",
    "    Compute Average Precision (AP) using 11-point interpolation.\n",
    "    \"\"\"\n",
    "    ap = 0.0\n",
    "    for t in np.linspace(0, 1, 11):\n",
    "        precisions_at_recall = [p for r, p in zip(recalls, precisions) if r >= t]\n",
    "        p_interp = max(precisions_at_recall) if precisions_at_recall else 0\n",
    "        ap += p_interp / 11.0\n",
    "    return ap\n",
    "\n",
    "\n",
    "def evaluate_class(gt_data, det_data, iou_thresh=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate detections for a single class.\n",
    "    \n",
    "    gt_data: dict mapping image_id -> list of ground truth boxes (each a list: [x, y, w, h])\n",
    "    det_data: list of tuples (image_id, box, confidence)\n",
    "    \n",
    "    Returns:\n",
    "        ap: Average Precision for the class.\n",
    "        recalls: Cumulative recall values.\n",
    "        precisions: Cumulative precision values.\n",
    "        max_f1: Maximum F1 score computed from the precision-recall curve.\n",
    "        avg_iou: Average IoU over true positive detections.\n",
    "    \"\"\"\n",
    "    npos = sum(len(boxes) for boxes in gt_data.values())\n",
    "    \n",
    "    # Create separate flags to mark detections (do not modify gt_data itself)\n",
    "    detected_flags = {img_id: [False] * len(boxes) for img_id, boxes in gt_data.items()}\n",
    "    \n",
    "    # Sort detections by confidence in descending order.\n",
    "    det_data = sorted(det_data, key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    tp = np.zeros(len(det_data))\n",
    "    fp = np.zeros(len(det_data))\n",
    "    tp_ious = []  # Store IoU for each true positive\n",
    "    \n",
    "    for i, (img_id, box, conf) in enumerate(det_data):\n",
    "        gt_boxes = gt_data.get(img_id, [])\n",
    "        best_iou = 0\n",
    "        best_idx = -1\n",
    "        for j, gt_box in enumerate(gt_boxes):\n",
    "            iou = compute_iou(box, gt_box)\n",
    "            if iou > best_iou:\n",
    "                best_iou = iou\n",
    "                best_idx = j\n",
    "        if best_iou >= iou_thresh:\n",
    "            if not detected_flags.get(img_id, [])[best_idx]:\n",
    "                tp[i] = 1\n",
    "                detected_flags[img_id][best_idx] = True\n",
    "                tp_ious.append(best_iou)\n",
    "            else:\n",
    "                fp[i] = 1\n",
    "        else:\n",
    "            fp[i] = 1\n",
    "    \n",
    "    tp_cumsum = np.cumsum(tp)\n",
    "    fp_cumsum = np.cumsum(fp)\n",
    "    recalls = tp_cumsum / npos if npos > 0 else np.zeros_like(tp_cumsum)\n",
    "    precisions = tp_cumsum / (tp_cumsum + fp_cumsum + 1e-6)\n",
    "    ap = compute_ap(recalls, precisions)\n",
    "    \n",
    "    # Compute F1 scores for each threshold and take the maximum F1.\n",
    "    f1_scores = [2 * p * r / (p + r) if (p + r) > 0 else 0 for p, r in zip(precisions, recalls)]\n",
    "    max_f1 = max(f1_scores) if f1_scores else 0\n",
    "    avg_iou = np.mean(tp_ious) if tp_ious else 0\n",
    "    \n",
    "    return ap, recalls, precisions, max_f1, avg_iou\n",
    "\n",
    "\n",
    "# --- Setup Directories ---\n",
    "gt_dir = os.path.abspath(\"../storage/logistics\")\n",
    "detections_dir1 = os.path.abspath(\"../detections\")\n",
    "detections_dir2 = os.path.abspath(\"../detections2\")\n",
    "\n",
    "# List ground truth annotation files (assumed to be .txt)\n",
    "gt_files = [f for f in os.listdir(gt_dir) if f.endswith(\".txt\")]\n",
    "\n",
    "# Data structures to store ground truth and detections by class.\n",
    "gt_by_class = {}     # {class: {image_id: [box, box, ...]}}\n",
    "det_by_class1 = {}   # {class: [(image_id, box, confidence), ...]}\n",
    "det_by_class2 = {}   # {class: [(image_id, box, confidence), ...]}\n",
    "\n",
    "# Populate ground truth data.\n",
    "for file in gt_files:\n",
    "    image_id = os.path.splitext(file)[0]\n",
    "    gt_path = os.path.join(gt_dir, file)\n",
    "    boxes, classes, _ = load_annotations(gt_path)\n",
    "    for box, cls in zip(boxes, classes):\n",
    "        if cls not in gt_by_class:\n",
    "            gt_by_class[cls] = {}\n",
    "        if image_id not in gt_by_class[cls]:\n",
    "            gt_by_class[cls][image_id] = []\n",
    "        gt_by_class[cls][image_id].append(box)\n",
    "\n",
    "\n",
    "def load_detections(file_path):\n",
    "    \"\"\"\n",
    "    Loads detections from a file.\n",
    "    Expected format per line:\n",
    "        x y w h confidence class_prob1 class_prob2 ... class_prob20\n",
    "\n",
    "    The final detection confidence is computed as:\n",
    "        final_confidence = confidence * max(class_probabilities)\n",
    "    and the predicted class is the index of the maximum class probability.\n",
    "    \"\"\"\n",
    "    boxes = []\n",
    "    classes = []\n",
    "    confidences = []\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 25:  # 4 for box, 1 for detection confidence, 20 for class probabilities\n",
    "                    # Parse bounding box coordinates\n",
    "                    x, y, w, h = map(float, parts[:4])\n",
    "                    # Parse the detection confidence score\n",
    "                    det_conf = float(parts[4])\n",
    "                    # Parse the 20 class probabilities\n",
    "                    class_probs = list(map(float, parts[5:25]))\n",
    "                    # Determine the predicted class and the corresponding class probability\n",
    "                    best_class = int(np.argmax(class_probs))\n",
    "                    best_class_prob = class_probs[best_class]\n",
    "                    # Compute the final confidence score\n",
    "                    final_conf = det_conf * best_class_prob\n",
    "                    \n",
    "                    boxes.append([x, y, w, h])\n",
    "                    classes.append(best_class)\n",
    "                    confidences.append(final_conf)\n",
    "    return boxes, classes, confidences\n",
    "\n",
    "\n",
    "# Populate detection data for Model 1.\n",
    "det_files1 = [f for f in os.listdir(detections_dir1) if f.endswith(\".txt\")]\n",
    "for file in det_files1:\n",
    "    image_id = os.path.splitext(file)[0]\n",
    "    det_path = os.path.join(detections_dir1, file)\n",
    "    boxes, classes, confidences = load_detections(det_path)\n",
    "    for box, cls, conf in zip(boxes, classes, confidences):\n",
    "        if cls not in det_by_class1:\n",
    "            det_by_class1[cls] = []\n",
    "        det_by_class1[cls].append((image_id, box, conf))\n",
    "\n",
    "# Populate detection data for Model 2.\n",
    "det_files2 = [f for f in os.listdir(detections_dir2) if f.endswith(\".txt\")]\n",
    "for file in det_files2:\n",
    "    image_id = os.path.splitext(file)[0]\n",
    "    det_path = os.path.join(detections_dir2, file)\n",
    "    boxes, classes, confidences = load_detections(det_path)\n",
    "    for box, cls, conf in zip(boxes, classes, confidences):\n",
    "        if cls not in det_by_class2:\n",
    "            det_by_class2[cls] = []\n",
    "        det_by_class2[cls].append((image_id, box, conf))\n",
    "\n",
    "\n",
    "# Load class names from the names file.\n",
    "class_names_path = os.path.abspath(\"../storage/yolo_models/logistics.names\")\n",
    "with open(class_names_path, \"r\") as f:\n",
    "    class_names = [line.strip() for line in f.readlines()]\n",
    "num_classes = len(class_names)\n",
    "\n",
    "\n",
    "def compute_map_all_classes(num_classes, gt_by_class, det_by_class, iou_thresh=0.5):\n",
    "    \"\"\"\n",
    "    For each class, compute AP, maximum F1, and average IoU.\n",
    "    Also, print per-class metrics and return overall metrics.\n",
    "    \"\"\"\n",
    "    aps = {}\n",
    "    f1_scores = {}\n",
    "    avg_ious = {}\n",
    "    for cls in range(num_classes):\n",
    "        # Retrieve ground truth for the class; if missing, use an empty dictionary.\n",
    "        gt_data = gt_by_class.get(cls, {})\n",
    "        # Make a deep copy so that detection flags remain independent.\n",
    "        gt_data_copy = {img_id: [box.copy() for box in boxes] for img_id, boxes in gt_data.items()}\n",
    "        if cls not in det_by_class:\n",
    "            ap = 0\n",
    "            f1 = 0\n",
    "            avg_iou = 0\n",
    "        else:\n",
    "            ap, rec, prec, f1, avg_iou = evaluate_class(gt_data_copy, det_by_class[cls], iou_thresh=iou_thresh)\n",
    "        aps[cls] = ap\n",
    "        f1_scores[cls] = f1\n",
    "        avg_ious[cls] = avg_iou\n",
    "        print(f\"Class {cls} ({class_names[cls]}): AP = {ap:.3f}, Max F1 = {f1:.3f}, Avg IoU = {avg_iou:.3f}\")\n",
    "    mAP = np.mean(list(aps.values()))\n",
    "    mF1 = np.mean(list(f1_scores.values()))\n",
    "    mAvgIoU = np.mean(list(avg_ious.values()))\n",
    "    return mAP, aps, mF1, mAvgIoU\n",
    "\n",
    "\n",
    "# --- Evaluate Model 1 ---\n",
    "print(\"------ Model 1 Evaluation ------\")\n",
    "mAP_model1, aps1, mF1_model1, mAvgIoU_model1 = compute_map_all_classes(num_classes, gt_by_class, det_by_class1, iou_thresh=0.000005)\n",
    "print(\"\\n--- Overall Metrics for Model 1 ---\")\n",
    "print(f\"Model 1 mAP: {mAP_model1:.3f}\")\n",
    "print(f\"Model 1 mF1: {mF1_model1:.3f}\")\n",
    "print(f\"Model 1 mAvgIoU: {mAvgIoU_model1:.3f}\")\n",
    "\n",
    "# --- Evaluate Model 2 ---\n",
    "print(\"\\n------ Model 2 Evaluation ------\")\n",
    "mAP_model2, aps2, mF1_model2, mAvgIoU_model2 = compute_map_all_classes(num_classes, gt_by_class, det_by_class2, iou_thresh=0.000005)\n",
    "print(\"\\n--- Overall Metrics for Model 2 ---\")\n",
    "print(f\"Model 2 mAP: {mAP_model2:.3f}\")\n",
    "print(f\"Model 2 mF1: {mF1_model2:.3f}\")\n",
    "print(f\"Model 2 mAvgIoU: {mAvgIoU_model2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Task B:**\n",
    "**Select the best model and state the reasons why this model is best suited for the TechTrack's system implementation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*You select Model X...because...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results, **Model 2** appears to be the better choice for TechTrack company's system. Here’s why:\n",
    "\n",
    "### Comparison of Metrics\n",
    "\n",
    "- **Mean Average Precision (mAP):**\n",
    "  - **Model 1:** 0.299  \n",
    "  - **Model 2:** 0.337  \n",
    "  mAP is calculated by computing the Average Precision (AP) for each class and then taking the average across all classes. AP is computed using 11-point interpolation over the precision-recall curve. Mathematically, if you denote the recall levels as \\( r_0, r_1, \\dots, r_{10} \\) and corresponding maximum precision values as \\( p(r_i) \\), then:\n",
    "\n",
    "  $$AP = \\frac{1}{11} \\sum_{i=0}^{10} p(r_i)$$\n",
    "  \n",
    "  A higher mAP indicates that, on average, the detector has a better trade-off between precision (the fraction of correct detections among all detections) and recall (the fraction of ground truth instances detected).\n",
    "\n",
    "- **Mean Maximum F1 Score (mF1):**\n",
    "  - **Model 1:** 0.416  \n",
    "  - **Model 2:** 0.457  \n",
    "  The F1 score is the harmonic mean of precision (P) and recall (R), computed as:\n",
    "  $$\n",
    "  F1 = \\frac{2 \\cdot P \\cdot R}{P + R}\n",
    "  $$\n",
    "  In this evaluation, the maximum F1 score is determined by calculating the F1 score at various points along the precision-recall curve and selecting the highest value. A higher maximum F1 indicates that the model can achieve a better balance between precision and recall at an optimal threshold.\n",
    "\n",
    "- **Mean Average IoU (mAvgIoU):**\n",
    "  - **Model 1:** 0.282  \n",
    "  - **Model 2:** 0.290  \n",
    "  IoU (Intersection over Union) measures the overlap between the predicted bounding box and the ground truth box:\n",
    "  $$\n",
    "  IoU = \\frac{\\text{Area of Intersection}}{\\text{Area of Union}}\n",
    "  $$\n",
    "  The average IoU is computed over all true positive detections. A higher IoU means that the bounding boxes predicted by the model are more accurately localized relative to the ground truth.\n",
    "\n",
    "### Why Model 2 is Best Suited for TechTrack\n",
    "\n",
    "1. **Overall Detection Accuracy (mAP):**  \n",
    "   Model 2 achieves a higher mAP (0.337 vs. 0.299), meaning it generally has better precision-recall performance across all object classes. For TechTrack, where reliable detection is crucial, a higher mAP translates to fewer missed detections and fewer false alarms.\n",
    "\n",
    "2. **Balanced Precision and Recall (Max F1 Score):**  \n",
    "   With a higher mean maximum F1 score (0.457 vs. 0.416), Model 2 is better at striking a balance between precision and recall. This is especially important in operational settings where both false positives (which can cause unnecessary interventions) and false negatives (which can lead to missed detections) have cost implications.\n",
    "\n",
    "3. **Better Localization (Avg IoU):**  \n",
    "   Although the difference is smaller, Model 2 has a slightly higher average IoU (0.290 vs. 0.282). This means that the bounding boxes it produces are generally more accurate, ensuring that the spatial localization of objects is reliable—a critical factor for tasks such as tracking and navigation in TechTrack's applications.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **mAP (Mean Average Precision):** Evaluates overall detection performance. Higher values indicate a better balance of precision and recall across classes.\n",
    "- **Max F1 Score:** Measures the best balance between precision and recall achievable by the model. It is the harmonic mean of precision and recall, where a higher score indicates a more effective detector.\n",
    "- **Avg IoU (Average Intersection over Union):** Assesses the quality of localization for true positive detections. Higher IoU values indicate better overlap between predicted and ground truth bounding boxes.\n",
    "\n",
    "Given that Model 2 outperforms Model 1 in all three key metrics—detection accuracy, balance between precision and recall, and localization quality—it is best suited for TechTrack company's system implementation. lHowever, I think something is wrong with my calculations. The performance of each model is suspiciously low. There might be some problem with the way I am looading the ground tgruth or loading the detections for each model that I ran."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (en605645)",
   "language": "python",
   "name": "en605645"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

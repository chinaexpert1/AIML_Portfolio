{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfbc8508-3567-42dc-a166-6381ec3bb78f",
   "metadata": {},
   "source": [
    "# Demo retrieval notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4456329-c812-444b-9340-fd2f43468c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building index for all-MiniLM-L6-v2 ...\n",
      "Building index for all-mpnet-base-v2 ...\n",
      "Performance for all-MiniLM-L6-v2:\n",
      "  Accuracy (true positive in top 5): 83.82%\n",
      "  Average rank of first true positive: 1.1713961407491487\n",
      "\n",
      "Performance for all-mpnet-base-v2:\n",
      "  Accuracy (true positive in top 5): 84.40%\n",
      "  Average rank of first true positive: 1.125140924464487\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Add the parent directory (one level up) to sys.path to access our modules\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from modules.extraction.preprocessing import DocumentProcessing\n",
    "from modules.extraction.embedding import Embedding\n",
    "from modules.retrieval.index.bruteforce import FaissBruteForce\n",
    "\n",
    "\n",
    "# Parameters\n",
    "STORAGE_DIRECTORY = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"storage\"))\n",
    "CHUNK_SIZE = 500         # characters per chunk for fixed-length chunking\n",
    "OVERLAP_SIZE = 2         # overlapping characters between chunks\n",
    "TOP_K = 5                # number of nearest neighbors to retrieve\n",
    "\n",
    "def build_index(embedding_model):\n",
    "    \"\"\"\n",
    "    Builds a FAISS index using the specified embedding model by processing all files\n",
    "    in the storage folder. Each file's article identifier is extracted from its filename.\n",
    "    \"\"\"\n",
    "    processing = DocumentProcessing()\n",
    "    embedding_instance = Embedding(embedding_model)\n",
    "    \n",
    "    # Get all files directly in the storage folder\n",
    "    document_files = glob.glob(os.path.join(STORAGE_DIRECTORY, \"*\"))\n",
    "    \n",
    "    all_embeddings = []\n",
    "    all_metadata = []\n",
    "    \n",
    "    for file_path in document_files:\n",
    "        # Extract the article identifier from the filename.\n",
    "        # For example, \"S08_set3_a4.txt.clean\" -> \"S08_set3_a4\"\n",
    "        base = os.path.basename(file_path)\n",
    "        article_id = base.replace('.txt.clean', '')\n",
    "        \n",
    "        # Use fixed-length chunking (implemented in preprocessing.py)\n",
    "        chunks = processing.fixed_length_chunking(file_path, chunk_size=CHUNK_SIZE, overlap_size=OVERLAP_SIZE)\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            if chunk.strip():\n",
    "                vector = embedding_instance.encode(chunk)\n",
    "                all_embeddings.append(vector)\n",
    "                # Save metadata as a dict that includes both the article id and the chunk text.\n",
    "                all_metadata.append({\"article\": article_id, \"chunk\": chunk})\n",
    "    \n",
    "    if not all_embeddings:\n",
    "        raise ValueError(\"No embeddings generated. Check your document files and chunking parameters.\")\n",
    "    \n",
    "    # Assume all embeddings have the same dimensionality.\n",
    "    dim = all_embeddings[0].shape[0]\n",
    "    \n",
    "    # Create the FAISS index using the FaissBruteForce class (defaulting to Euclidean metric)\n",
    "    index = FaissBruteForce(dim, metric='euclidean')\n",
    "    index.add_embeddings(all_embeddings, all_metadata)\n",
    "    return index\n",
    "\n",
    "# Build two indices â€“ one for each embedding model.\n",
    "print(\"Building index for all-MiniLM-L6-v2 ...\")\n",
    "index_mini = build_index(\"all-MiniLM-L6-v2\")\n",
    "print(\"Building index for all-mpnet-base-v2 ...\")\n",
    "index_mpnet = build_index(\"all-mpnet-base-v2\")\n",
    "\n",
    "# Load the questions TSV file (located one level up in qa_resources/)\n",
    "qa_file = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"qa_resources\", \"question.tsv\"))\n",
    "questions_df = pd.read_csv(qa_file, sep=\"\\t\")\n",
    "\n",
    "# We assume the TSV file has at least these two columns: \"Question\" and \"ArticleFile\"\n",
    "def evaluate_index(index, embedding_model):\n",
    "    embedding_instance = Embedding(embedding_model)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    ranks = []  # to record ranking positions of the first true positive\n",
    "    \n",
    "    for _, row in questions_df.iterrows():\n",
    "        # Convert question to a string in case it is not already one.\n",
    "        question = str(row[\"Question\"])\n",
    "        target_article = row[\"ArticleFile\"]  # e.g. \"S08_set3_a4\"\n",
    "        \n",
    "        # Encode the question.\n",
    "        q_vector = embedding_instance.encode(question)\n",
    "        q_vector = np.array(q_vector).astype('float32').reshape(1, -1)\n",
    "        \n",
    "        # Retrieve top K neighbors from the index.\n",
    "        distances, indices = index.index.search(q_vector, TOP_K)\n",
    "        \n",
    "        # Look up metadata for the retrieved indices.\n",
    "        retrieved_metadata = [index.metadata[i] for i in indices[0]]\n",
    "        \n",
    "        # Check ranking: record the rank of the first chunk whose 'article' matches target_article.\n",
    "        rank = None\n",
    "        for i, meta in enumerate(retrieved_metadata):\n",
    "            if meta[\"article\"] == target_article:\n",
    "                rank = i + 1  # ranks are 1-indexed\n",
    "                break\n",
    "        \n",
    "        if rank is not None:\n",
    "            correct += 1\n",
    "            ranks.append(rank)\n",
    "        else:\n",
    "            ranks.append(None)\n",
    "        total += 1\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    valid_ranks = [r for r in ranks if r is not None]\n",
    "    avg_rank = sum(valid_ranks) / len(valid_ranks) if valid_ranks else None\n",
    "    return accuracy, avg_rank\n",
    "\n",
    "# Evaluate both indices.\n",
    "accuracy_mini, avg_rank_mini = evaluate_index(index_mini, \"all-MiniLM-L6-v2\")\n",
    "accuracy_mpnet, avg_rank_mpnet = evaluate_index(index_mpnet, \"all-mpnet-base-v2\")\n",
    "\n",
    "print(\"Performance for all-MiniLM-L6-v2:\")\n",
    "print(\"  Accuracy (true positive in top {}): {:.2f}%\".format(TOP_K, accuracy_mini * 100))\n",
    "print(\"  Average rank of first true positive:\", avg_rank_mini)\n",
    "\n",
    "print(\"\\nPerformance for all-mpnet-base-v2:\")\n",
    "print(\"  Accuracy (true positive in top {}): {:.2f}%\".format(TOP_K, accuracy_mpnet * 100))\n",
    "print(\"  Average rank of first true positive:\", avg_rank_mpnet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f9db13-9bb2-46cc-924b-83e5c0a98bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss2",
   "language": "python",
   "name": "faiss2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "190c5330-64ff-4dd4-82ef-dc48a3064dee",
   "metadata": {},
   "source": [
    "# EDA notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ba4b293-be42-4a03-8c51-cd50d453d280",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ace_tools'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mstats\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mace_tools\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ace_tools'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pandas.plotting import scatter_matrix\n",
    "from modules.data.raw_data_handler import RawDataHandler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32505c63-0336-48f9-a7b7-888e73175db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logging\n",
    "log_messages = []\n",
    "\n",
    "def log(msg):\n",
    "    \"\"\"Logs the message to the console and stores it in log_messages.\"\"\"\n",
    "    print(msg)\n",
    "    log_messages.append(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84df099a-1aeb-4fa3-9c5b-c0dda90e1fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the correct dataset path\n",
    "dataset_dir = os.path.join(\"storage\", \"datasets\", \"dataset_stratified_SMOTE_50_20250308073917.csv\")\n",
    "\n",
    "# Attempt to load the dataset using the corrected path\n",
    "try:\n",
    "    cleaned_data = pd.read_csv(dataset_dir)\n",
    "    log(\"Dataset loaded successfully using the corrected path.\")\n",
    "except FileNotFoundError:\n",
    "    log(f\"Error: File not found at {dataset_dir}. Please verify the file location.\")\n",
    "    raise\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89726b12-3abd-4805-9a52-438d201660cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Ensure required columns exist\n",
    "required_columns = {'unix_time', 'amt', 'category', 'hour', 'fraud_label'}\n",
    "missing_columns = required_columns - set(cleaned_data.columns)\n",
    "if missing_columns:\n",
    "    log(f\"Error: Missing required columns: {missing_columns}\")\n",
    "    raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "\n",
    "log(f\"Dataset shape: {cleaned_data.shape}\")\n",
    "log(f\"Columns present: {list(cleaned_data.columns)}\")\n",
    "\n",
    "# Convert unix_time to datetime\n",
    "cleaned_data['datetime'] = pd.to_datetime(cleaned_data['unix_time'], unit='s')\n",
    "log(\"Converted unix_time to datetime.\")\n",
    "\n",
    "# Compute Z-score for transaction amount\n",
    "cleaned_data['z_score_transaction_amount'] = stats.zscore(cleaned_data['amt'])\n",
    "log(\"Computed Z-score for transaction amount.\")\n",
    "\n",
    "# Fixing transaction count computation using searchsorted for proper rolling window calculations\n",
    "log(\"Fixing transaction count computations...\")\n",
    "\n",
    "# Ensure data is sorted by datetime\n",
    "cleaned_data = cleaned_data.sort_values(by='datetime')\n",
    "\n",
    "# Convert datetime to numeric timestamp for efficient search\n",
    "timestamps = cleaned_data['datetime'].astype(np.int64) // 10**9  # Convert to seconds\n",
    "\n",
    "# Function to compute transactions in last X seconds\n",
    "def count_past_transactions(time_series, window_seconds):\n",
    "    indices = np.searchsorted(time_series, time_series - window_seconds, side='left')\n",
    "    return np.arange(len(time_series)) - indices\n",
    "\n",
    "# Compute transaction frequencies\n",
    "cleaned_data['transactions_in_last_1hr'] = count_past_transactions(timestamps, 3600)\n",
    "cleaned_data['transactions_in_last_24hrs'] = count_past_transactions(timestamps, 86400)\n",
    "cleaned_data['transactions_in_last_7_days'] = count_past_transactions(timestamps, 604800)\n",
    "\n",
    "log(\"Recomputed transactions in the last 1 hour, 24 hours, and 7 days.\")\n",
    "\n",
    "# Display dataset with engineered features\n",
    "import ace_tools as tools\n",
    "tools.display_dataframe_to_user(name=\"Updated Fraud Detection Features\", dataframe=cleaned_data)\n",
    "\n",
    "# Visualize how each feature isolates fraud transactions\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "features = [\n",
    "    'z_score_transaction_amount', 'transactions_in_last_1hr', 'transactions_in_last_24hrs',\n",
    "    'transactions_in_last_7_days', 'uncommon_category', 'is_night_transaction',\n",
    "    'avg_time_between_transactions_last_hour', 'kl_divergence_spending_patterns'\n",
    "]\n",
    "\n",
    "for ax, feature in zip(axes.flatten(), features):\n",
    "    ax.hist(cleaned_data[cleaned_data['fraud_label'] == 0][feature], bins=30, alpha=0.5, label='Legit', density=True)\n",
    "    ax.hist(cleaned_data[cleaned_data['fraud_label'] == 1][feature], bins=30, alpha=0.5, label='Fraud', density=True)\n",
    "    ax.set_title(feature)\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "log(\"Feature distributions successfully plotted.\")\n",
    "\n",
    "\n",
    "# Flag uncommon categories for users\n",
    "category_counts = cleaned_data.groupby('category')['category'].transform('count')\n",
    "cleaned_data['uncommon_category'] = (category_counts < category_counts.quantile(0.25)).astype(int)\n",
    "log(\"Flagged uncommon categories for users.\")\n",
    "\n",
    "# Flag night transactions\n",
    "cleaned_data['is_night_transaction'] = ((cleaned_data['hour'] >= 0) & (cleaned_data['hour'] < 6)).astype(int)\n",
    "log(\"Flagged night transactions.\")\n",
    "\n",
    "# Compute average time between transactions in last hour\n",
    "cleaned_data['avg_time_between_transactions_last_hour'] = cleaned_data.groupby('category')['datetime'].transform(\n",
    "    lambda x: x.diff().dt.total_seconds().rolling(60).mean()\n",
    ")\n",
    "log(\"Computed average time between transactions in the last hour.\")\n",
    "\n",
    "# Compute KL Divergence on Spending Patterns\n",
    "def compute_kl_divergence(data):\n",
    "    past_amounts = data.shift(1).dropna()\n",
    "    if past_amounts.empty:\n",
    "        return np.nan\n",
    "    recent_dist = np.histogram(data, bins=10, density=True)[0] + 1e-9  # Avoid zero probabilities\n",
    "    past_dist = np.histogram(past_amounts, bins=10, density=True)[0] + 1e-9\n",
    "    return stats.entropy(recent_dist, past_dist)\n",
    "\n",
    "cleaned_data['kl_divergence_spending_patterns'] = cleaned_data.groupby('category')['amt'].transform(compute_kl_divergence)\n",
    "log(\"Computed KL divergence on spending patterns.\")\n",
    "\n",
    "# Display dataset with engineered features\n",
    "import ace_tools as tools\n",
    "tools.display_dataframe_to_user(name=\"Engineered Features for Fraud Detection\", dataframe=cleaned_data)\n",
    "\n",
    "# Visualize how each feature isolates fraud transactions\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "features = [\n",
    "    'z_score_transaction_amount', 'transactions_in_last_1hr', 'transactions_in_last_24hrs',\n",
    "    'transactions_in_last_7_days', 'uncommon_category', 'is_night_transaction',\n",
    "    'avg_time_between_transactions_last_hour', 'kl_divergence_spending_patterns'\n",
    "]\n",
    "\n",
    "for ax, feature in zip(axes.flatten(), features):\n",
    "    ax.hist(cleaned_data[cleaned_data['fraud_label'] == 0][feature], bins=30, alpha=0.5, label='Legit', density=True)\n",
    "    ax.hist(cleaned_data[cleaned_data['fraud_label'] == 1][feature], bins=30, alpha=0.5, label='Fraud', density=True)\n",
    "    ax.set_title(feature)\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Log visualization success\n",
    "log(\"Feature distributions plotted successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485992b3-eee3-4411-96ee-d94521494fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_log(data, logtype = \"default\"):\n",
    "    \"\"\"Log request and response data in a TXT file with a timestamp filename.\"\"\"\n",
    "    log_type = log_type\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    log_dir = os.path.join(\"logs\")\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    log_filename = os.path.join(log_dir, f\"log_{log_type}_{timestamp}.txt\")\n",
    "\n",
    "    # If data is a dict and might contain non-string items (like DataFrames), \n",
    "    # we convert it to a string representation. Using json.dumps with indent \n",
    "    # can produce a nice formatted string.\n",
    "    # The default=str argument will call str() on any non-serializable objects.\n",
    "    log_str = json.dumps(data, indent=4, default=str)\n",
    "\n",
    "    with open(log_filename, \"w\") as f:\n",
    "        f.write(log_str)\n",
    "\n",
    "save_log(log_messages, log_type= \"feature_engineering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f5d85d-b540-4ea7-8bc7-71b449da33d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def verify_smote_distribution(df):\n",
    "    \"\"\"\n",
    "    logs the count and proportion of each fraud_label value in the DataFrame.\n",
    "    \"\"\"\n",
    "    counts = df['fraud_label'].value_counts()\n",
    "    total = counts.sum()\n",
    "\n",
    "    log(\"Distribution of fraud labels after SMOTE:\")\n",
    "    for label, count in counts.items():\n",
    "        log(f\"  fraud_label={label}: {count} rows ({count/total:.2%})\")\n",
    "    log(\"Use  /create_dataset?partition_type=stratified&minority_percent=75 to change the SMOTE distribution\")\n",
    "# Example usage (assuming your SMOTE-processed DataFrame is called sample_data):\n",
    "verify_smote_distribution(cleaned_data)\n",
    "# --------------------------\n",
    "# Feature Engineering\n",
    "# --------------------------\n",
    "\n",
    "# 1. Create binary encoding for weekdays vs. weekends.\n",
    "if \"day_of_week\" in cleaned_data.columns:\n",
    "    # Check the type of day_of_week; if numeric, assume 0=Monday ... 6=Sunday.\n",
    "    if pd.api.types.is_numeric_dtype(cleaned_data[\"day_of_week\"]):\n",
    "        cleaned_data[\"is_weekend\"] = cleaned_data[\"day_of_week\"].apply(lambda x: 1 if x in [5, 6] else 0)\n",
    "    else:\n",
    "        # Assuming the column contains day names as strings.\n",
    "        cleaned_data[\"is_weekend\"] = cleaned_data[\"day_of_week\"].apply(lambda x: 1 if x in [\"Saturday\", \"Sunday\"] else 0)\n",
    "    log(\"Engineered 'is_weekend' feature based on 'day_of_week'.\")\n",
    "else:\n",
    "    log(\"'day_of_week' column not found; cannot engineer 'is_weekend' feature.\")\n",
    "\n",
    "# 2. Discretize the 'hour' feature into 4-hour bins.\n",
    "if \"hour\" in cleaned_data.columns:\n",
    "    bins = [0, 6, 12, 18, 24]\n",
    "    labels = [f\"{bins[i]}-{bins[i+1]-1}\" for i in range(len(bins)-1)]\n",
    "    cleaned_data[\"hour_bin\"] = pd.cut(cleaned_data[\"hour\"], bins=bins, right=False, labels=labels, include_lowest=True)\n",
    "    cleaned_data[\"hour_bin\"] = cleaned_data[\"hour_bin\"].cat.codes  # Numeric codes for correlation analysis\n",
    "    log(\"Engineered 'hour_bin' feature from 'hour'.\")\n",
    "else:\n",
    "    log(\"'hour' column not found; cannot engineer 'hour_bin' feature.\")\n",
    "\n",
    "# 3. Drop fraud records where city_pop is greater than 50,000\n",
    "if \"city_pop\" in cleaned_data.columns:\n",
    "    before_drop = cleaned_data.shape[0]\n",
    "    cleaned_data = cleaned_data[~((cleaned_data[\"fraud_label\"] == 1) & (cleaned_data[\"city_pop\"] > 50000))]\n",
    "    after_drop = cleaned_data.shape[0]\n",
    "    log(f\"Dropped {before_drop - after_drop} fraud records with city_pop > 50000.\")\n",
    "else:\n",
    "    log(\"Column 'city_pop' not found in the dataset.\")\n",
    "\n",
    "# 4. Drop rows with amt > 1500\n",
    "if \"amt\" in cleaned_data.columns:\n",
    "    before_drop = cleaned_data.shape[0]\n",
    "    cleaned_data = cleaned_data[cleaned_data[\"amt\"] <= 1500]\n",
    "    after_drop = cleaned_data.shape[0]\n",
    "    log(f\"\\nDropped {before_drop - after_drop} rows where 'amt' > 1500.\")\n",
    "else:\n",
    "    log(\"\\nColumn 'amt' not found in the dataset. No rows dropped based on 'amt'.\")\n",
    "\n",
    "# 5. log transform amt columnm, adding log_amt column\n",
    "if \"amt\" in cleaned_data.columns:\n",
    "    cleaned_data[\"log_amt\"] = np.log1p(cleaned_data[\"amt\"])\n",
    "    log(\"Created 'log_amt' feature using logarithmic transformation.\")\n",
    "else:\n",
    "    log(\"'amt' column not found in the dataset.\")\n",
    "\n",
    "\n",
    "# log the remaining columns \n",
    "log(\"Remaining columns in the dataset before trans_date_trans_time creates <5 min feature:\")\n",
    "log(list(cleaned_data.columns))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define tolerance and thresholds for log_amt_special\n",
    "tolerance = 0.1\n",
    "thresholds = [3.0, 5.6, 5.8, 6.6, 6.8]\n",
    "\n",
    "def is_near_threshold(x, thresholds, tol):\n",
    "    return any(abs(x - t) < tol for t in thresholds)\n",
    "\n",
    "# 1. Create a feature for log(amt) being near specified thresholds\n",
    "if \"log_amt\" in cleaned_data.columns:\n",
    "    cleaned_data[\"log_amt_special\"] = cleaned_data[\"log_amt\"].apply(lambda x: 1 if is_near_threshold(x, thresholds, tolerance) else 0)\n",
    "    log(\"Created 'log_amt_special' feature indicating if log(amt) is near one of the thresholds:\")\n",
    "    log(thresholds)\n",
    "else:\n",
    "    log(\"'log_amt' column not found; cannot create 'log_amt_special' feature.\")\n",
    "\n",
    "\n",
    "\n",
    "# 2. Create a feature for when merchant transaction frequency is between 1500 and 2200\n",
    "if \"merchant_txn_count\" in cleaned_data.columns:\n",
    "    cleaned_data[\"merchant_txn_range\"] = ((cleaned_data[\"merchant_txn_count\"] >= 1500) & (cleaned_data[\"merchant_txn_count\"] <= 2200)).astype(int)\n",
    "    log(\"Created 'merchant_txn_range' feature indicating if merchant_txn_count is between 1500 and 2200.\")\n",
    "else:\n",
    "    log(\"'merchant_txn_count' column not found; cannot create 'merchant_txn_range' feature.\")\n",
    "\n",
    "# 3. create a feature of the top 4 categories\n",
    "# Specify the number of top categories to consider as \"popular\"\n",
    "num_top = 4\n",
    "\n",
    "if \"category\" in cleaned_data.columns and \"fraud_label\" in cleaned_data.columns:\n",
    "    # Filter only the fraud cases\n",
    "    fraud_data = cleaned_data[cleaned_data[\"fraud_label\"] == 1]\n",
    "    \n",
    "    # Count the frequency of each category in fraud cases\n",
    "    top_categories = fraud_data[\"category\"].value_counts().head(num_top).index.tolist()\n",
    "    \n",
    "    # Create a new binary feature that is 1 if the record's category is one of the top fraud categories\n",
    "    cleaned_data[\"popular_category_fraud\"] = cleaned_data[\"category\"].apply(lambda x: 1 if x in top_categories else 0)\n",
    "    \n",
    "    print(f\"Created 'popular_category_fraud' feature using top {num_top} fraud categories:\", top_categories)\n",
    "else:\n",
    "    print(\"Required columns ('category', 'fraud_label') not found in the dataset.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# log the remaining columns after dropping\n",
    "log(\"Remaining columns in the dataset:\")\n",
    "log(list(cleaned_data.columns))\n",
    "\n",
    "# --------------------------\n",
    "# Plotting Engineered Features by Fraud Label\n",
    "# --------------------------\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Plot for log_amt\n",
    "if \"log_amt\" in cleaned_data.columns:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.histplot(data=cleaned_data, x=\"log_amt\", hue=\"fraud_label\", bins=30, multiple=\"stack\")\n",
    "    plt.title(\"Distribution of Log(amt) by Fraud Label\")\n",
    "    plt.xlabel(\"Log(amt)\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Filter the dataset for fraudulent transactions (fraud_label = 1)\n",
    "fraud_data = cleaned_data[cleaned_data[\"fraud_label\"] == 1]\n",
    "\n",
    "# Plot for log_amt\n",
    "if \"log_amt\" in fraud_data.columns:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.histplot(data=fraud_data, x=\"log_amt\", bins=30)\n",
    "    plt.title(\"Distribution of Log(amt) for Fraudulent Transactions\")\n",
    "    plt.xlabel(\"Log(amt)\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Visualizations for Custom Features by Fraud Label\n",
    "# --------------------------\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot for log_amt_special\n",
    "if \"log_amt_special\" in cleaned_data.columns:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.countplot(x=\"log_amt_special\", hue=\"fraud_label\", data=cleaned_data)\n",
    "    plt.title(\"Count of log_amt_special Feature by Fraud Label\")\n",
    "    plt.xlabel(\"log_amt_special (0: not near threshold, 1: near threshold)\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f34e88-b310-49fc-992b-2469ea1b53a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Separate features and target from the cleaned_data\n",
    "X = cleaned_data.drop(\"fraud_label\", axis=1)\n",
    "y = cleaned_data[\"fraud_label\"]\n",
    "\n",
    "# Identify non-numeric columns in X\n",
    "non_numeric_cols = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "log(\"Non-numeric columns in X:\")\n",
    "log(non_numeric_cols)\n",
    "\n",
    "# Select only numeric features for model fitting\n",
    "X_numeric = X.select_dtypes(include=[np.number])\n",
    "log(\"Using numeric features:\")\n",
    "log(X_numeric.columns.tolist())\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Step 1: Train a Random Forest and get feature importances\n",
    "#rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "#rf.fit(X_numeric, y.loc[X_numeric.index])\n",
    "#importances = pd.Series(rf.feature_importances_, index=X_numeric.columns)\n",
    "#log(\"Random Forest Feature Importances:\")\n",
    "#log(importances.sort_values(ascending=False))\n",
    "\n",
    "# Step 2: Use RFE with a Random Forest estimator to select features\n",
    "#rfe = RFE(estimator=RandomForestClassifier(n_estimators=100, random_state=42), n_features_to_select=10)\n",
    "#rfe.fit(X_numeric, y.loc[X_numeric.index])\n",
    "#selected_features = X_numeric.columns[rfe.support_]\n",
    "#log(\"Selected features via RFE:\")\n",
    "#log(selected_features)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Additional Feature Engineering for Selection Methods\n",
    "# --------------------------\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "\n",
    "# Find and log columns that have non-numeric values\n",
    "non_numeric_cols = cleaned_data.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "log(\"Columns with non-numeric values:\")\n",
    "log(non_numeric_cols)\n",
    "\n",
    "\n",
    "# Separate features and target from the full dataset (for selection purposes)\n",
    "X = cleaned_data.drop(\"fraud_label\", axis=1)\n",
    "y = cleaned_data[\"fraud_label\"]\n",
    "\n",
    "# Select only numeric features to avoid type promotion issues (drop datetime and non-numeric columns)\n",
    "X_numeric = X.select_dtypes(include=[np.number])\n",
    "log(\"Numeric features used for mutual information:\")\n",
    "log(X_numeric.columns.tolist())\n",
    "\n",
    "# Use mutual information to score features using only numeric features\n",
    "selector = SelectKBest(score_func=mutual_info_classif, k='all')\n",
    "selector.fit(X_numeric, y.loc[X_numeric.index])\n",
    "mi_scores = pd.Series(selector.scores_, index=X_numeric.columns)\n",
    "log(\"Mutual Information Scores (Numeric Columns Only):\")\n",
    "log(mi_scores.sort_values(ascending=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (en605645)",
   "language": "python",
   "name": "en605645"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

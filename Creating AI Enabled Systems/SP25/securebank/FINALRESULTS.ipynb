{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a240a65-0f75-43d4-9eec-2757bf0c29e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Get the project root directory\n",
    "root_dir = Path().resolve()\n",
    "if root_dir.name == 'your_project':\n",
    "    sys.path.append(str(root_dir))\n",
    "else:\n",
    "    # If notebook is in a subdirectory\n",
    "    parent_dir = root_dir.parent\n",
    "    sys.path.append(str(parent_dir))\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pandas.plotting import scatter_matrix\n",
    "from modules.data.raw_data_handler import RawDataHandler\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "log_messages = []\n",
    "\n",
    "def log(msg):\n",
    "    \"\"\"\n",
    "    logs the message to the console\n",
    "    AND stores it in the log_messages list for later use in the response.\n",
    "    \"\"\"\n",
    "    print(msg)\n",
    "    log_messages.append(msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd747a28-6e62-4f1f-9980-08fb92a05723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "Total records:\n",
      "540507\n",
      "Total features:\n",
      "14\n",
      "Remaining columns in the dataset:\n",
      "['trans_date_trans_time', 'unix_time', 'merchant', 'category', 'amt', 'fraud_label', 'zip', 'city_pop', 'job', 'day_of_week', 'hour', 'minute', 'second', 'day_date', 'month_date']\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = os.path.join( \"storage\", \"merged_data.csv\")\n",
    "\n",
    "# Define the path to your CSV file (adjust the path as needed)\n",
    "csv_file_path = dataset_dir\n",
    "\n",
    "# Read the CSV file into a DataFrame named cleaned_data\n",
    "cleaned_data = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Verify that the target column \"fraud_label\" exists\n",
    "if \"fraud_label\" not in cleaned_data.columns:\n",
    "    raise ValueError(\"The CSV file does not contain the required 'fraud_label' column.\")\n",
    "\n",
    "# Optionally, separate features and target:\n",
    "X = cleaned_data.drop(\"fraud_label\", axis=1)\n",
    "y = cleaned_data[\"fraud_label\"]\n",
    "\n",
    "log(\"Dataset loaded successfully!\")\n",
    "log(\"Total records:\")\n",
    "log(cleaned_data.shape[0])\n",
    "log(\"Total features:\")\n",
    "log(X.shape[1])\n",
    "\n",
    "# log the remaining columns after dropping\n",
    "log(\"Remaining columns in the dataset:\")\n",
    "log(list(cleaned_data.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d2b34f8-3611-42d0-b5e0-aaf045f7216c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['trans_num', 'index_x', 'merch_lat', 'merch_long', 'index_y', 'first', 'last', 'sex', 'street', 'city', 'lat', 'long', 'dob', 'year_date'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m columns_to_drop \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      2\u001b[0m    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrans_num\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex_x\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrans_date_trans_time\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmerch_lat\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmerch_long\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex_y\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlast\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msex\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstreet\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlong\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdob\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear_date\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Drop the columns\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m cleaned_data \u001b[38;5;241m=\u001b[39m \u001b[43mcleaned_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns_to_drop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Verify the columns were dropped\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(cleaned_data\u001b[38;5;241m.\u001b[39mcolumns)\n",
      "File \u001b[1;32m~\\.conda\\envs\\en605645\\Lib\\site-packages\\pandas\\core\\frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5446\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5583\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5587\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5588\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5589\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\en605645\\Lib\\site-packages\\pandas\\core\\generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32m~\\.conda\\envs\\en605645\\Lib\\site-packages\\pandas\\core\\generic.py:4830\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4828\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4829\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4830\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4831\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4833\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4834\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\.conda\\envs\\en605645\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['trans_num', 'index_x', 'merch_lat', 'merch_long', 'index_y', 'first', 'last', 'sex', 'street', 'city', 'lat', 'long', 'dob', 'year_date'] not found in axis\""
     ]
    }
   ],
   "source": [
    "\n",
    "columns_to_drop = [\n",
    "   'trans_num', 'index_x', 'trans_date_trans_time', 'merch_lat', 'merch_long', 'index_y', 'first', 'last', 'sex', 'street', 'city', 'lat', 'long', 'dob', 'year_date']\n",
    "# Drop the columns\n",
    "cleaned_data = cleaned_data.drop(columns=columns_to_drop)\n",
    "\n",
    "# Verify the columns were dropped\n",
    "print(cleaned_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96f6794-7f60-4ec3-bed2-465d6a3685d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a summary of your data\n",
    "print(cleaned_data.info())\n",
    "\n",
    "# Check the first few rows to see potential missing value patterns\n",
    "print(cleaned_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af55e6f-c75d-4c8e-badc-f4eada7f6242",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Columns to encode\n",
    "columns_to_encode = ['merchant', 'category', 'state', 'job', 'day_of_week', 'month_date']\n",
    "\n",
    "# Create a dictionary to store the encoders\n",
    "label_encoders = {}\n",
    "\n",
    "# Create a copy of the dataframe to avoid modifying the original\n",
    "encoded_data = cleaned_data.copy()\n",
    "\n",
    "# Apply label encoding to each column\n",
    "for column in columns_to_encode:\n",
    "    le = LabelEncoder()\n",
    "    encoded_data[column] = le.fit_transform(encoded_data[column])\n",
    "    \n",
    "    # Store the encoder for future reference (optional)\n",
    "    label_encoders[column] = le\n",
    "    \n",
    "    # Print mapping information (optional)\n",
    "    print(f\"Encoded {column}: {len(le.classes_)} unique values\")\n",
    "\n",
    "# Verify the encoding worked\n",
    "print(\"\\nSample of encoded data:\")\n",
    "print(encoded_data[columns_to_encode].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13561724-b401-4072-8be9-026d7de4c2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have already created encoded_data with 'fraud_label' column\n",
    "# If 'fraud_label' was dropped earlier, make sure you have it in your dataframe\n",
    "\n",
    "# Filter for fraud cases\n",
    "fraud_data = encoded_data[encoded_data['fraud_label'] == 1]\n",
    "non_fraud_data = encoded_data[encoded_data['fraud_label'] == 0]\n",
    "\n",
    "# Set up the plots\n",
    "plt.figure(figsize=(16, 20))\n",
    "columns_to_analyze = ['merchant', 'category', 'state', 'job', 'day_of_week', 'month_date']\n",
    "\n",
    "# Plot distribution for each column\n",
    "for i, column in enumerate(columns_to_analyze):\n",
    "    plt.subplot(3, 2, i+1)\n",
    "    \n",
    "    # For categorical columns (the encoded ones)\n",
    "    if column in columns_to_encode:\n",
    "        # Count frequency of each category in fraud cases\n",
    "        value_counts = fraud_data[column].value_counts().sort_index()\n",
    "        \n",
    "        # Get total counts for calculating percentages\n",
    "        total_counts = encoded_data[column].value_counts().sort_index()\n",
    "        fraud_percentage = (value_counts / total_counts) * 100\n",
    "        \n",
    "        # Plot both the raw count and the percentage\n",
    "        ax1 = plt.gca()\n",
    "        bars = ax1.bar(value_counts.index, value_counts.values, alpha=0.6, color='crimson')\n",
    "        ax1.set_ylabel('Count of Fraud Cases', color='crimson')\n",
    "        ax1.tick_params(axis='y', labelcolor='crimson')\n",
    "        \n",
    "        # Add percentage axis\n",
    "        ax2 = ax1.twinx()\n",
    "        line = ax2.plot(fraud_percentage.index, fraud_percentage.values, 'b-', marker='o')\n",
    "        ax2.set_ylabel('% of Category that is Fraud', color='blue')\n",
    "        ax2.tick_params(axis='y', labelcolor='blue')\n",
    "        \n",
    "        # If we have too many categories, limit x-axis labels\n",
    "        if len(value_counts) > 10:\n",
    "            plt.xticks(rotation=90)\n",
    "            \n",
    "            # Only show some of the labels to avoid overcrowding\n",
    "            step = max(1, len(value_counts) // 10)\n",
    "            plt.xticks(value_counts.index[::step])\n",
    "        \n",
    "        # Try to map back to original labels if available\n",
    "        if column in label_encoders:\n",
    "            # Create a mapping dictionary\n",
    "            label_map = {i: label for i, label in enumerate(label_encoders[column].classes_)}\n",
    "            \n",
    "            # Add some labels for reference in the plot\n",
    "            for j, bar in enumerate(bars):\n",
    "                if j % max(1, len(bars) // 5) == 0:  # Add labels for every 5th bar\n",
    "                    ax1.text(bar.get_x() + bar.get_width()/2, 5,\n",
    "                            f\"{label_map.get(j, j)}\", \n",
    "                            ha='center', va='bottom', rotation=90, fontsize=8)\n",
    "    \n",
    "    # For numerical columns, use histograms\n",
    "    else:\n",
    "        sns.histplot(data=fraud_data, x=column, color='crimson', kde=True, alpha=0.6)\n",
    "        \n",
    "    plt.title(f'Distribution of {column} in Fraud Cases')\n",
    "    plt.tight_layout()\n",
    "\n",
    "# Add a plot for top fraud merchants if merchant is in the dataset\n",
    "if 'merchant' in columns_to_analyze:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    top_fraud_merchants = fraud_data['merchant'].value_counts().head(20)\n",
    "    \n",
    "    # Calculate fraud percentage for these merchants\n",
    "    merchant_counts = encoded_data['merchant'].value_counts()\n",
    "    top_fraud_merchant_indexes = top_fraud_merchants.index\n",
    "    top_merchant_total_counts = merchant_counts[top_fraud_merchant_indexes]\n",
    "    fraud_percentage = (top_fraud_merchants / top_merchant_total_counts) * 100\n",
    "    \n",
    "    # Sort by percentage\n",
    "    fraud_percentage = fraud_percentage.sort_values(ascending=False)\n",
    "    \n",
    "    # Create the bar plot\n",
    "    ax = fraud_percentage.plot(kind='bar', color='darkred')\n",
    "    \n",
    "    # Try to map back to original merchant names\n",
    "    if 'merchant' in label_encoders:\n",
    "        label_map = {i: label for i, label in enumerate(label_encoders['merchant'].classes_)}\n",
    "        labels = [label_map.get(idx, idx) for idx in fraud_percentage.index]\n",
    "        ax.set_xticklabels(labels, rotation=90)\n",
    "    \n",
    "    plt.title('Top Merchants by Fraud Percentage')\n",
    "    plt.ylabel('Fraud Percentage')\n",
    "    plt.tight_layout()\n",
    "\n",
    "# Plot fraud by day of week if available\n",
    "if 'day_of_week' in columns_to_analyze:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Get the day of week distribution\n",
    "    dow_fraud = fraud_data['day_of_week'].value_counts().reindex(range(7), fill_value=0)\n",
    "    \n",
    "    # Get total counts and calculate percentage\n",
    "    dow_total = encoded_data['day_of_week'].value_counts().reindex(range(7), fill_value=0)\n",
    "    dow_percentage = (dow_fraud / dow_total) * 100\n",
    "    \n",
    "    # Create a bar plot\n",
    "    ax = plt.subplot(111)\n",
    "    bars = ax.bar(dow_fraud.index, dow_fraud.values, alpha=0.7, color='purple')\n",
    "    \n",
    "    # Add percentage line\n",
    "    ax2 = ax.twinx()\n",
    "    line = ax2.plot(dow_percentage.index, dow_percentage.values, 'g-', marker='o', linewidth=2)\n",
    "    ax2.set_ylabel('Fraud Percentage', color='green')\n",
    "    \n",
    "    # Map numeric days to actual day names\n",
    "    days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    if 'day_of_week' in label_encoders:\n",
    "        # Get the mapping\n",
    "        label_map = {i: label for i, label in enumerate(label_encoders['day_of_week'].classes_)}\n",
    "        # Try to infer the correct order\n",
    "        if set(label_map.values()) == set(days):\n",
    "            # If days are stored as strings, map correctly\n",
    "            day_order = {day: i for i, day in enumerate(days)}\n",
    "            ordered_labels = sorted(label_map.items(), key=lambda x: day_order.get(x[1], 0))\n",
    "            plt.xticks([x[0] for x in ordered_labels], [x[1] for x in ordered_labels])\n",
    "    \n",
    "    plt.title('Fraud Distribution by Day of Week')\n",
    "    plt.xlabel('Day of Week')\n",
    "    ax.set_ylabel('Count of Fraud Cases')\n",
    "    plt.tight_layout()\n",
    "\n",
    "# Plot time-based patterns if available\n",
    "if all(col in encoded_data.columns for col in ['hour', 'month_date']):\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    \n",
    "    # Hour distribution\n",
    "    plt.subplot(121)\n",
    "    if 'hour' in encoded_data.columns:\n",
    "        hour_fraud = fraud_data['hour'].value_counts().sort_index()\n",
    "        total_hour = encoded_data['hour'].value_counts().sort_index()\n",
    "        hour_percentage = (hour_fraud / total_hour) * 100\n",
    "        \n",
    "        ax1 = plt.gca()\n",
    "        bars = ax1.bar(hour_fraud.index, hour_fraud.values, alpha=0.6, color='darkblue')\n",
    "        ax1.set_xlabel('Hour of Day')\n",
    "        ax1.set_ylabel('Count of Fraud Cases', color='darkblue')\n",
    "        \n",
    "        ax2 = ax1.twinx()\n",
    "        line = ax2.plot(hour_percentage.index, hour_percentage.values, 'r-', marker='o')\n",
    "        ax2.set_ylabel('Fraud Percentage', color='red')\n",
    "        \n",
    "        plt.title('Fraud Distribution by Hour of Day')\n",
    "    \n",
    "    # Month distribution\n",
    "    plt.subplot(122)\n",
    "    if 'month_date' in encoded_data.columns:\n",
    "        month_fraud = fraud_data['month_date'].value_counts().sort_index()\n",
    "        total_month = encoded_data['month_date'].value_counts().sort_index()\n",
    "        month_percentage = (month_fraud / total_month) * 100\n",
    "        \n",
    "        ax1 = plt.gca()\n",
    "        bars = ax1.bar(month_fraud.index, month_fraud.values, alpha=0.6, color='darkgreen')\n",
    "        ax1.set_ylabel('Count of Fraud Cases', color='darkgreen')\n",
    "        \n",
    "        # Try to map to month names\n",
    "        month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "        if 'month_date' in label_encoders:\n",
    "            label_map = {i: label for i, label in enumerate(label_encoders['month_date'].classes_)}\n",
    "            plt.xticks(range(len(label_map)), [label_map.get(i, i) for i in range(len(label_map))])\n",
    "        \n",
    "        ax2 = ax1.twinx()\n",
    "        line = ax2.plot(month_percentage.index, month_percentage.values, 'm-', marker='o')\n",
    "        ax2.set_ylabel('Fraud Percentage', color='magenta')\n",
    "        \n",
    "        plt.title('Fraud Distribution by Month')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "# Add one more insightful visualization: Heatmap of fraud rate by state and category if available\n",
    "if all(col in encoded_data.columns for col in ['state', 'category']):\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Create a pivot table with fraud rates\n",
    "    pivot_data = encoded_data.pivot_table(\n",
    "        index='state', \n",
    "        columns='category',\n",
    "        values='fraud_label',\n",
    "        aggfunc='mean'  # This gives us the fraud rate\n",
    "    ) * 100  # Convert to percentage\n",
    "    \n",
    "    # Create the heatmap\n",
    "    ax = sns.heatmap(pivot_data, annot=True, cmap='YlOrRd', fmt='.2f', linewidths=.5)\n",
    "    \n",
    "    # Try to map indices and columns back to original labels\n",
    "    if 'state' in label_encoders and 'category' in label_encoders:\n",
    "        state_map = {i: label for i, label in enumerate(label_encoders['state'].classes_)}\n",
    "        category_map = {i: label for i, label in enumerate(label_encoders['category'].classes_)}\n",
    "        \n",
    "        # Get current labels\n",
    "        row_labels = [state_map.get(i, i) for i in pivot_data.index]\n",
    "        col_labels = [category_map.get(i, i) for i in pivot_data.columns]\n",
    "        \n",
    "        ax.set_yticklabels(row_labels)\n",
    "        ax.set_xticklabels(col_labels, rotation=90)\n",
    "    \n",
    "    plt.title('Fraud Rate (%) by State and Category')\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def univariate_bivariate_analysis(encoded_data, max_features=6):\n",
    "    \"\"\"\n",
    "    Perform univariate and bivariate analysis on a label-encoded dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - encoded_data: DataFrame containing features and a 'fraud_label' column.\n",
    "    - max_features: Max number of features to plot.\n",
    "    \"\"\"\n",
    "    if 'fraud_label' not in encoded_data.columns:\n",
    "        raise ValueError(\"Dataset must contain a 'fraud_label' column.\")\n",
    "\n",
    "    # Split data\n",
    "    fraud_data = encoded_data[encoded_data['fraud_label'] == 1]\n",
    "    non_fraud_data = encoded_data[encoded_data['fraud_label'] == 0]\n",
    "\n",
    "    # Infer numeric features (excluding the label)\n",
    "    numeric_cols = encoded_data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    numeric_cols = [col for col in numeric_cols if col != 'fraud_label']\n",
    "    numeric_cols = numeric_cols[:max_features]  # limit to avoid clutter\n",
    "\n",
    "    print(f\"Running analysis on {len(numeric_cols)} numerical features...\")\n",
    "\n",
    "    for col in numeric_cols:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "        # KDE Plot\n",
    "        sns.kdeplot(non_fraud_data[col], ax=axes[0], label='Non-Fraud', fill=True, color='blue')\n",
    "        sns.kdeplot(fraud_data[col], ax=axes[0], label='Fraud', fill=True, color='red')\n",
    "        axes[0].set_title(f'Distribution of {col}')\n",
    "        axes[0].legend()\n",
    "\n",
    "        # Box Plot\n",
    "        sns.boxplot(data=encoded_data, x='fraud_label', y=col, ax=axes[1])\n",
    "        axes[1].set_xticklabels(['Non-Fraud', 'Fraud'])\n",
    "        axes[1].set_title(f'{col} by Fraud Label')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "univariate_bivariate_analysis(encoded_data, max_features=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfb2b7f-f611-4dcf-9b56-716e3b0eb26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = encoded_data\n",
    "\n",
    "# Create a 5% stratified sample\n",
    "# This preserves the same proportion of fraud/non-fraud cases as in the original data\n",
    "sample_size = 0.9  # 10% of the data\n",
    "\n",
    "# Split the data - the \"test\" set will be our 5% sample\n",
    "_, sample_data = train_test_split(\n",
    "    df,                   # Your original dataframe\n",
    "    test_size=sample_size,\n",
    "    random_state=42,      # For reproducibility\n",
    "    stratify=df['fraud_label']  # Stratify based on fraud label\n",
    ")\n",
    "\n",
    "# Verify the sample size\n",
    "print(f\"Original dataset size: {len(df)}\")\n",
    "print(f\"Sample dataset size: {len(sample_data)} ({len(sample_data)/len(df)*100:.2f}%)\")\n",
    "\n",
    "# Verify that the fraud distribution is maintained\n",
    "original_fraud_rate = df['fraud_label'].mean() * 100\n",
    "sample_fraud_rate = sample_data['fraud_label'].mean() * 100\n",
    "\n",
    "print(f\"Original fraud rate: {original_fraud_rate:.2f}%\")\n",
    "print(f\"Sample fraud rate: {sample_fraud_rate:.2f}%\")\n",
    "\n",
    "# Use the sample_data dataframe for your feature engineering\n",
    "encoded_data = sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8db33b-4c81-4c8e-9af8-dabd8f5726f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Assuming we have the encoded_data DataFrame with all columns mentioned earlier\n",
    "# First, let's create a copy to work with\n",
    "df = encoded_data.copy()\n",
    "\n",
    "# Helper function to get top N categories with highest fraud rates\n",
    "def get_top_fraud_categories(df, column, n=10, min_count=50):\n",
    "    # Group by the column and calculate fraud rate and total count\n",
    "    fraud_rates = df.groupby(column).agg({\n",
    "        'fraud_label': ['mean', 'count']\n",
    "    })\n",
    "    \n",
    "    # Flatten the multi-index columns\n",
    "    fraud_rates.columns = ['fraud_rate', 'count']\n",
    "    \n",
    "    # Filter out categories with too few transactions\n",
    "    fraud_rates = fraud_rates[fraud_rates['count'] >= min_count]\n",
    "    \n",
    "    # Sort by fraud rate in descending order and get top N\n",
    "    top_categories = fraud_rates.sort_values('fraud_rate', ascending=False).head(n).index.tolist()\n",
    "    \n",
    "    print(f\"Top {n} {column} categories by fraud rate:\")\n",
    "    for cat in top_categories:\n",
    "        rate = fraud_rates.loc[cat, 'fraud_rate'] * 100\n",
    "        count = fraud_rates.loc[cat, 'count']\n",
    "        print(f\"  {cat}: {rate:.2f}% fraud rate ({count} transactions)\")\n",
    "    \n",
    "    return top_categories\n",
    "\n",
    "# 1. Create binary features for top merchants with fraud\n",
    "print(\"\\n=== Top Merchants with Fraud ===\")\n",
    "top_fraud_merchants = get_top_fraud_categories(df, 'merchant', n=5, min_count=5)\n",
    "df['is_top5_fraud_merchant'] = df['merchant'].isin(top_fraud_merchants).astype(int)\n",
    "\n",
    "# 2. Create binary features for top categories with fraud\n",
    "print(\"\\n=== Top Categories with Fraud ===\")\n",
    "top_fraud_categories = get_top_fraud_categories(df, 'category', n=6, min_count=20)\n",
    "df['is_top6_fraud_category'] = df['category'].isin(top_fraud_categories).astype(int)\n",
    "\n",
    "# 3. Create binary features for top jobs with fraud\n",
    "print(\"\\n=== Top Jobs with Fraud ===\")\n",
    "top_fraud_jobs = get_top_fraud_categories(df, 'job', n=10, min_count=20)\n",
    "df['is_top10_fraud_job'] = df['job'].isin(top_fraud_jobs).astype(int)\n",
    "\n",
    "# 4. Create binary features for top hours with fraud\n",
    "print(\"\\n=== Top Hours with Fraud ===\")\n",
    "top_fraud_hours = get_top_fraud_categories(df, 'hour', n=6)\n",
    "df['is_top6_fraud_hour'] = df['hour'].isin(top_fraud_hours).astype(int)\n",
    "\n",
    "# 5. Create binary features for top months with fraud\n",
    "print(\"\\n=== Top Months with Fraud ===\")\n",
    "top_fraud_months = get_top_fraud_categories(df, 'month_date', n=4)\n",
    "df['is_top4_fraud_month'] = df['month_date'].isin(top_fraud_months).astype(int)\n",
    "\n",
    "# 6. Create binary features for top days with fraud\n",
    "print(\"\\n=== Top Days with Fraud ===\")\n",
    "top_fraud_days = get_top_fraud_categories(df, 'day_of_week', n=3)\n",
    "df['is_top3_fraud_day'] = df['day_of_week'].isin(top_fraud_days).astype(int)\n",
    "\n",
    "# 7. Create binary features for top zip codes with fraud\n",
    "print(\"\\n=== Top ZIP Codes with Fraud ===\")\n",
    "top_fraud_zips = get_top_fraud_categories(df, 'zip', n=2, min_count=10)\n",
    "df['is_top2_fraud_zip'] = df['zip'].isin(top_fraud_zips).astype(int)\n",
    "\n",
    "# 8. Log transform the amount column\n",
    "df['log_amt'] = np.log1p(df['amt'])  # log1p to handle zero amounts\n",
    "\n",
    "# 9. Create binary feature for suspicious amounts\n",
    "# Find transaction amounts that are more common in fraud cases\n",
    "print(\"\\n=== Suspicious Transaction Amounts ===\")\n",
    "# Bin amounts into ranges\n",
    "df['amt_bin'] = pd.qcut(df['amt'], 20, duplicates='drop')\n",
    "\n",
    "# Calculate fraud rate for each bin\n",
    "amt_fraud_rates = df.groupby('amt_bin').agg({\n",
    "    'fraud_label': ['mean', 'count']\n",
    "})\n",
    "amt_fraud_rates.columns = ['fraud_rate', 'count']\n",
    "\n",
    "# Get overall fraud rate\n",
    "overall_fraud_rate = df['fraud_label'].mean()\n",
    "\n",
    "# Find bins with significantly higher fraud rates\n",
    "significant_bins = amt_fraud_rates[amt_fraud_rates['fraud_rate'] > 2 * overall_fraud_rate].index.tolist()\n",
    "df['is_suspicious_amt'] = df['amt_bin'].isin(significant_bins).astype(int)\n",
    "\n",
    "# Display the suspicious amount ranges\n",
    "print(f\"Overall fraud rate: {overall_fraud_rate*100:.2f}%\")\n",
    "print(\"Amount ranges with high fraud rates:\")\n",
    "for bin_range in significant_bins:\n",
    "    rate = amt_fraud_rates.loc[bin_range, 'fraud_rate'] * 100\n",
    "    count = amt_fraud_rates.loc[bin_range, 'count']\n",
    "    print(f\"  {bin_range}: {rate:.2f}% fraud rate ({count} transactions)\")\n",
    "\n",
    "# Remove the temporary bin column\n",
    "df.drop('amt_bin', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Keep only engineered features and drop original columns\n",
    "\n",
    "\n",
    "engineered_features = [\n",
    "    # Original features...\n",
    "    'is_top6_fraud_category',\n",
    "    'is_top10_fraud_job',\n",
    "    'is_top6_fraud_hour',\n",
    "    'is_top4_fraud_month',\n",
    "    'is_top3_fraud_day',\n",
    "    'log_amt',\n",
    "    'is_suspicious_amt',\n",
    "    'fraud_label'  # Keep the target\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Create the final engineered dataframe\n",
    "final_df = df[engineered_features]\n",
    "\n",
    "\n",
    "# drop\n",
    "\n",
    "# After creating all your engineered features in final_df\n",
    "\n",
    "# List of columns to keep (only binary features and fraud_label)\n",
    "binary_features = [\n",
    "    'is_top6_fraud_category',\n",
    "    'is_top10_fraud_job',\n",
    "    'is_top6_fraud_hour',\n",
    "    'is_top4_fraud_month',\n",
    "    'is_top3_fraud_day',\n",
    "    'is_suspicious_amt',\n",
    "    'fraud_label'  # Keep the target variable\n",
    "]\n",
    "\n",
    "# Explicitly drop the non-binary features\n",
    "#columns_to_drop = [\n",
    "         # Continuous log-transformed amount\n",
    "#    'tx_last_hour', # Count of transactions in last hour\n",
    "#    'tx_last_day',\n",
    "#    'log_amt' \n",
    "     # Add any other non-binary features that might be in your dataframe\n",
    "#]\n",
    "\n",
    "# Create the final binary-only dataframe\n",
    "#binary_df = final_df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Alternatively, you can directly select only the binary columns\n",
    "# binary_df = final_df[binary_features]\n",
    "\n",
    "# Verify that only binary features remain\n",
    "#print(\"Final binary dataset shape:\", binary_df.shape)\n",
    "#print(\"Columns in final binary dataset:\", binary_df.columns.tolist())\n",
    "\n",
    "# Check that all remaining features are truly binary (0 or 1 values only)\n",
    "#for col in binary_df.columns:\n",
    "#    if col != 'fraud_label':  # Skip the target variable\n",
    "#        unique_values = binary_df[col].unique()\n",
    "#        is_binary = set(unique_values).issubset({0, 1})\n",
    "#        print(f\"{col}: Binary = {is_binary}, Unique values = {unique_values}\")\n",
    "\n",
    "# Now you can proceed with your train/test split using binary_df\n",
    "#final_df = binary_df  # Update your working dataframe to only include binary features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Show the correlation between engineered features and fraud\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation = final_df.corr()['fraud_label'].sort_values(ascending=False)\n",
    "correlation = correlation.drop('fraud_label')  # Remove self-correlation\n",
    "\n",
    "# Plot correlation of features with fraud\n",
    "plt.barh(range(len(correlation)), correlation.values)\n",
    "plt.yticks(range(len(correlation)), correlation.index)\n",
    "plt.xlabel('Correlation with Fraud')\n",
    "plt.title('Engineered Features Correlation with Fraud')\n",
    "plt.axvline(x=0, color='gray', linestyle='--')\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display summary statistics of the engineered features\n",
    "print(\"\\n=== Engineered Features Summary ===\")\n",
    "print(final_df.describe())\n",
    "\n",
    "# Display feature value distributions grouped by fraud/non-fraud\n",
    "plt.figure(figsize=(18, 12))\n",
    "for i, feature in enumerate(engineered_features[:-1]):  # Exclude fraud_label\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    sns.boxplot(x='fraud_label', y=feature, data=final_df)\n",
    "    plt.title(feature)\n",
    "    plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFinal engineered dataset shape:\", final_df.shape)\n",
    "print(\"Binary features created:\", sum(1 for col in final_df.columns if col.startswith('is_') or col.startswith('has_') or col.startswith('unusual_')))\n",
    "\n",
    "# Save the final dataframe if needed\n",
    "# final_df.to_csv('engineered_fraud_features.csv', index=False)\n",
    "\n",
    "# Return the final dataframe with engineered features\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a3be23-fe3f-463b-a778-3c52ef58e598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "def prepare_data(df, test_size=0.1, val_size=0.1, undersample_ratio=1.0, random_state=42):\n",
    "    \"\"\"\n",
    "    Split data into train, validation and test sets.\n",
    "    Apply Random Undersampling to balance the training set (if undersample_ratio < 1.0), \n",
    "    then threshold binary features (except for 'log amt'). Finally, drop the bottom 15 \n",
    "    features with the lowest absolute correlation with fraud_label.\n",
    "    \"\"\"\n",
    "    # Print dataset information\n",
    "    print(\"\\n=== Dataset Information ===\")\n",
    "    print(f\"DataFrame shape: {df.shape}\")\n",
    "    print(\"\\nColumn datatypes:\")\n",
    "    print(df.dtypes)\n",
    "    print(\"\\nSample data:\")\n",
    "    print(df.head())\n",
    "    print(\"\\nClass distribution:\")\n",
    "    print(df['fraud_label'].value_counts())\n",
    "    print(f\"Fraud percentage: {df['fraud_label'].mean() * 100:.2f}%\")\n",
    "    \n",
    "    # Split features and target\n",
    "    X = df.drop('fraud_label', axis=1)\n",
    "    y = df['fraud_label']\n",
    "    \n",
    "    # First split: training+validation vs test\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Second split: training vs validation\n",
    "    adjusted_val_size = val_size / (1 - test_size)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=adjusted_val_size, random_state=random_state, stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    # Print split sizes and class distributions\n",
    "    print(\"\\n=== Data Split Information ===\")\n",
    "    print(f\"Training set: {len(X_train)} samples ({len(X_train)/len(df)*100:.1f}%)\")\n",
    "    print(f\"Validation set: {len(X_val)} samples ({len(X_val)/len(df)*100:.1f}%)\")\n",
    "    print(f\"Test set: {len(X_test)} samples ({len(X_test)/len(df)*100:.1f}%)\")\n",
    "    print(\"\\nClass distribution in splits:\")\n",
    "    print(f\"  Training: {y_train.mean()*100:.2f}% fraud\")\n",
    "    print(f\"  Validation: {y_val.mean()*100:.2f}% fraud\")\n",
    "    print(f\"  Test: {y_test.mean()*100:.2f}% fraud\")\n",
    "    \n",
    "\n",
    "    # Apply Undersampling if undersample_ratio < 1.0, otherwise skip it.\n",
    "    if undersample_ratio < 1.0:\n",
    "        print(\"\\n=== Applying Random Undersampling ===\")\n",
    "        n_minority = (y_train == 1).sum()\n",
    "        n_majority = int(n_minority / undersample_ratio)\n",
    "\n",
    "        sampling_strategy = {0: n_majority, 1: n_minority}\n",
    "        rus = RandomUnderSampler(sampling_strategy=sampling_strategy, random_state=random_state)\n",
    "        X_train_resampled, y_train_resampled = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "        # Threshold binary features (exclude log_amt)\n",
    "        X_train_resampled = pd.DataFrame(X_train_resampled, columns=X_train.columns)\n",
    "        binary_cols = [col for col in X_train.columns if col != 'log_amt']\n",
    "        X_train_resampled[binary_cols] = (X_train_resampled[binary_cols] >= 0.5).astype(int)\n",
    "    else:\n",
    "        print(\"\\n=== Skipping Undersampling ===\")\n",
    "        X_train_resampled, y_train_resampled = X_train.copy(), y_train.copy()\n",
    "\n",
    "    # Print class distributions\n",
    "    print(\"Original training class distribution:\")\n",
    "    print(pd.Series(y_train).value_counts())\n",
    "    print(f\"Original fraud percentage: {y_train.mean()*100:.2f}%\")\n",
    "    print(\"\\nResampled training class distribution:\")\n",
    "    print(pd.Series(y_train_resampled).value_counts())\n",
    "    print(f\"Resampled fraud percentage: {np.mean(y_train_resampled)*100:.2f}%\")\n",
    "    \n",
    "    # Print class distributions\n",
    "    print(\"Original training class distribution:\")\n",
    "    print(pd.Series(y_train).value_counts())\n",
    "    print(f\"Original fraud percentage: {y_train.mean()*100:.2f}%\")\n",
    "    print(\"\\nResampled training class distribution:\")\n",
    "    print(pd.Series(y_train_resampled).value_counts())\n",
    "    print(f\"Resampled fraud percentage: {np.mean(y_train_resampled)*100:.2f}%\")\n",
    "    \n",
    "    # Visualize class distribution before and after SMOTE (or skip)\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.countplot(x=y_train)\n",
    "    plt.title('Before SMOTE (Original)')\n",
    "    plt.xlabel('Fraud Label')\n",
    "    plt.ylabel('Count')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.countplot(x=y_train_resampled)\n",
    "    plt.title('After SMOTE' if smote_ratio > 0 else 'No SMOTE Applied')\n",
    "    plt.xlabel('Fraud Label')\n",
    "    plt.ylabel('Count')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Compute correlations on the resampled training set\n",
    "    print(\"\\n=== Feature Correlations After Resampling ===\")\n",
    "    resampled_df = X_train_resampled.copy()\n",
    "    resampled_df['fraud_label'] = y_train_resampled\n",
    "    correlations = resampled_df.corr()['fraud_label'].drop('fraud_label')\n",
    "    print(\"\\nAll feature correlations with fraud_label:\")\n",
    "    print(correlations.sort_values(ascending=False))\n",
    "    \n",
    "    # Identify and drop the bottom X features (least correlated in absolute value)\n",
    "    abs_corr = correlations.abs()\n",
    "    features_to_drop = abs_corr.sort_values(ascending=True).head(18).index.tolist()\n",
    "    print(\"\\nDropping the bottom 18 features (least correlated with fraud_label):\")\n",
    "    print(features_to_drop)\n",
    "    \n",
    "    # Drop these features from all datasets\n",
    "    X_train = X_train.drop(columns=features_to_drop)\n",
    "    X_val = X_val.drop(columns=features_to_drop)\n",
    "    X_test = X_test.drop(columns=features_to_drop)\n",
    "    X_train_resampled = X_train_resampled.drop(columns=features_to_drop)\n",
    "    \n",
    "    # Plot correlation heatmap of the resampled data after dropping features\n",
    "    resampled_df = X_train_resampled.copy()\n",
    "    resampled_df['fraud_label'] = y_train_resampled\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(resampled_df.corr(), annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)\n",
    "    plt.title('Correlation Matrix After SMOTE and Feature Dropping')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, X_train_resampled, y_train_resampled\n",
    "\n",
    "# Example usage:\n",
    "# Assuming final_df is your dataframe with a 'fraud_label' column and a 'log amt' column.\n",
    "# Example usage:\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, X_train_resampled, y_train_resampled = prepare_data(\n",
    "    final_df, \n",
    "    test_size=0.1, \n",
    "    val_size=0.1, \n",
    "    undersample_ratio=0.8  # Keep 1 fraud for every 5 non-frauds\n",
    ")\n",
    "\n",
    "print(\"\\n=== Final Dataset Sizes ===\")\n",
    "print(f\"Original training set: {X_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"SMOTE-enhanced training set: {X_train_resampled.shape if X_train_resampled is not None else 'None'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f440187-647e-4f9e-a5e2-f28d30c5f3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch and related imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_curve, roc_auc_score,\n",
    "    precision_recall_curve, average_precision_score, precision_score,\n",
    "    recall_score, f1_score\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "#############################################\n",
    "# Define a PyTorch Neural Network Wrapper with Dropout and BatchNorm\n",
    "#############################################\n",
    "class TorchNNClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, input_dim, hidden_dim=32, num_hidden_layers=1, lr=0.001, \n",
    "                 num_epochs=10, batch_size=64, dropout=0.5, random_state=42, verbose=0):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.lr = lr\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout = dropout\n",
    "        self.random_state = random_state\n",
    "        self.verbose = verbose\n",
    "        self._build_model()\n",
    "    \n",
    "    def _build_model(self):\n",
    "        layers = []\n",
    "        # First hidden layer: Linear -> BatchNorm -> ReLU -> Dropout\n",
    "        layers.append(nn.Linear(self.input_dim, self.hidden_dim))\n",
    "        layers.append(nn.BatchNorm1d(self.hidden_dim))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Dropout(p=self.dropout))\n",
    "        # Additional hidden layers (if any)\n",
    "        for _ in range(self.num_hidden_layers - 1):\n",
    "            layers.append(nn.Linear(self.hidden_dim, self.hidden_dim))\n",
    "            layers.append(nn.BatchNorm1d(self.hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(p=self.dropout))\n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(self.hidden_dim, 1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Ensure reproducibility\n",
    "        torch.manual_seed(self.random_state)\n",
    "        self._build_model()  # reinitialize model each time\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X_tensor = torch.FloatTensor(X.values)\n",
    "        else:\n",
    "            X_tensor = torch.FloatTensor(X)\n",
    "        if isinstance(y, pd.Series) or isinstance(y, pd.DataFrame):\n",
    "            y_tensor = torch.FloatTensor(y.values).view(-1, 1)\n",
    "        else:\n",
    "            y_tensor = torch.FloatTensor(y).view(-1, 1)\n",
    "        dataset = torch.utils.data.TensorDataset(X_tensor, y_tensor)\n",
    "        loader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        \n",
    "        self.model.train()\n",
    "        for epoch in range(self.num_epochs):\n",
    "            for batch_X, batch_y in loader:\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(batch_X)\n",
    "                loss = self.criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            if self.verbose:\n",
    "                print(f\"Epoch {epoch+1}/{self.num_epochs}, Loss: {loss.item():.4f}\")\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        self.model.eval()\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X_tensor = torch.FloatTensor(X.values)\n",
    "        else:\n",
    "            X_tensor = torch.FloatTensor(X)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(X_tensor)\n",
    "            probs = torch.sigmoid(outputs).numpy().flatten()\n",
    "        # Return probability for class 0 and 1\n",
    "        return np.vstack([1 - probs, probs]).T\n",
    "    \n",
    "    def predict(self, X):\n",
    "        probs = self.predict_proba(X)[:, 1]\n",
    "        return (probs >= 0.5).astype(int)\n",
    "    \n",
    "    #############################################\n",
    "    # Define the evaluation function with grid search\n",
    "    #############################################\n",
    "    def evaluate_fraud_models(X_train, X_val, X_test, y_train, y_val, y_test, \n",
    "                              X_train_resampled=None, y_train_resampled=None,\n",
    "                              smote_ratio=None, verbose=True):\n",
    "        \"\"\"\n",
    "        Train and evaluate fraud detection models (XGBoost, Random Forest, and a PyTorch Neural Network)\n",
    "        using grid search for hyperparameter tuning and threshold optimization.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test : training, validation, and test splits\n",
    "        X_train_resampled, y_train_resampled : optional SMOTE-resampled training data\n",
    "        smote_ratio : the SMOTE ratio used (for tracking in results)\n",
    "        verbose : whether to print detailed output and display plots\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        Dictionary containing model performance metrics and results.\n",
    "        \"\"\"\n",
    "        # Determine whether to use resampled data\n",
    "        if X_train_resampled is not None and y_train_resampled is not None:\n",
    "            X_train_use = X_train_resampled\n",
    "            y_train_use = y_train_resampled\n",
    "            print(f\"Using SMOTE-resampled training data (ratio: {smote_ratio})\")\n",
    "        else:\n",
    "            X_train_use = X_train\n",
    "            y_train_use = y_train\n",
    "            print(\"Using original training data (no SMOTE)\")\n",
    "        \n",
    "        # Define default hyperparameter grids\n",
    "        xgb_param_grid = {\n",
    "            'n_estimators': [100, 150, 200, 300, 400],\n",
    "            'max_depth': [10, 15, 20, 25, 30, 50, 100],\n",
    "            'learning_rate': [0.1]\n",
    "        }\n",
    "        \n",
    "        rf_param_grid = {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [None, 10, 15, 20, 30],\n",
    "            'min_samples_split': [2, 3, 5, 7, 10, 12]\n",
    "        }\n",
    "    \n",
    "    nn_param_grid = {\n",
    "        'hidden_dim': [40, 50, 60, 70, 100],\n",
    "        'num_hidden_layers': [1],\n",
    "        'lr': [0.1],\n",
    "        'num_epochs': [10, 15],\n",
    "        'batch_size': [32]\n",
    "    }\n",
    "    \n",
    "    # Print hyperparameter grids\n",
    "    print(\"\\nDefault hyperparameter grids for grid search:\")\n",
    "    print(\"XGBoost:\", xgb_param_grid)\n",
    "    print(\"Random Forest:\", rf_param_grid)\n",
    "    print(\"Neural Network:\", nn_param_grid)\n",
    "    \n",
    "    # Initialize models  note that for the NN we must supply input_dim from X_train\n",
    "    models = {\n",
    "        'XGBoost': xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
    "        'Random Forest': RandomForestClassifier(class_weight='balanced', random_state=42),\n",
    "        'Neural Network': TorchNNClassifier(input_dim=X_train.shape[1], random_state=42, verbose=0)\n",
    "    }\n",
    "    \n",
    "    # Map each model to its corresponding hyperparameter grid\n",
    "    param_grids = {\n",
    "        'XGBoost': xgb_param_grid,\n",
    "        'Random Forest': rf_param_grid,\n",
    "        'Neural Network': nn_param_grid\n",
    "    }\n",
    "    \n",
    "    # Dictionary to store results\n",
    "    results = {\n",
    "        'smote_ratio': smote_ratio,\n",
    "        'models': {},\n",
    "        'best_model': None,\n",
    "        'best_threshold': None,\n",
    "        'best_f1': 0,\n",
    "        'thresholds': {}\n",
    "    }\n",
    "    \n",
    "    # For each model, run grid search and evaluate\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n==== Grid Search and Training for {name} ====\")\n",
    "        grid = GridSearchCV(estimator=model, param_grid=param_grids[name],\n",
    "                            scoring='f1', cv=5, n_jobs=-1, verbose=0)\n",
    "        grid.fit(X_train_use, y_train_use)\n",
    "        best_model = grid.best_estimator_\n",
    "        print(f\"Best hyperparameters for {name}: {grid.best_params_}\")\n",
    "        \n",
    "        # Cross validation on training data (using the best estimator)\n",
    "        cv_scores = cross_val_score(best_model, X_train_use, y_train_use, cv=5, scoring='f1')\n",
    "        print(f\"Cross-validation F1 scores: {cv_scores}\")\n",
    "        print(f\"Mean CV F1 score: {cv_scores.mean():.3f}\")\n",
    "        \n",
    "        # Predict on validation set\n",
    "        y_val_pred = best_model.predict(X_val)\n",
    "        y_val_prob = best_model.predict_proba(X_val)[:, 1]\n",
    "        \n",
    "        # Calculate metrics on validation set\n",
    "        val_precision = precision_score(y_val, y_val_pred)\n",
    "        val_recall = recall_score(y_val, y_val_pred)\n",
    "        val_f1 = f1_score(y_val, y_val_pred)\n",
    "        val_auc = roc_auc_score(y_val, y_val_prob)\n",
    "        val_avg_precision = average_precision_score(y_val, y_val_prob)\n",
    "        \n",
    "        print(f\"\\nValidation set metrics (default threshold):\")\n",
    "        print(f\"Precision: {val_precision:.3f}\")\n",
    "        print(f\"Recall: {val_recall:.3f}\")\n",
    "        print(f\"F1 Score: {val_f1:.3f}\")\n",
    "        print(f\"ROC AUC: {val_auc:.3f}\")\n",
    "        print(f\"PR AUC: {val_avg_precision:.3f}\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"\\nClassification Report (Validation Set):\")\n",
    "            print(classification_report(y_val, y_val_pred))\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            cm = confusion_matrix(y_val, y_val_pred)\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "            plt.title(f'{name} Confusion Matrix (Validation)')\n",
    "            plt.ylabel('True Label')\n",
    "            plt.xlabel('Predicted Label')\n",
    "            plt.show()\n",
    "        \n",
    "        # For the evaluate_fraud_models function in the document:\n",
    "\n",
    "\n",
    "        \n",
    "        # With this new optimization code:\n",
    "        # Threshold optimization on validation set using precision-recall curve\n",
    "        precisions, recalls, thresholds = precision_recall_curve(y_val, y_val_prob)\n",
    "        thresholds = np.append(thresholds, 1.0)\n",
    "        f1_scores = []\n",
    "        for i in range(len(thresholds)):\n",
    "            if precisions[i] + recalls[i] > 0:\n",
    "                f1_temp = 2 * (precisions[i] * recalls[i]) / (precisions[i] + recalls[i])\n",
    "            else:\n",
    "                f1_temp = 0\n",
    "            f1_scores.append(f1_temp)\n",
    "        \n",
    "        # Find indices where precision is at least 30%\n",
    "        min_precision = 0.30  # 30% precision requirement\n",
    "        valid_indices = np.where(precisions >= min_precision)[0]\n",
    "        \n",
    "        if len(valid_indices) > 0:\n",
    "            # Find the threshold that maximizes recall while maintaining at least 30% precision\n",
    "            best_recall_idx = valid_indices[np.argmax(recalls[valid_indices])]\n",
    "            best_threshold = thresholds[best_recall_idx]\n",
    "            best_precision = precisions[best_recall_idx]\n",
    "            best_recall = recalls[best_recall_idx]\n",
    "            best_f1 = f1_scores[best_recall_idx]\n",
    "            \n",
    "            print(f\"\\nOptimal threshold for {name}: {best_threshold:.3f} (maximizing recall with 30% precision)\")\n",
    "            print(f\"At this threshold - Precision: {best_precision:.3f}, Recall: {best_recall:.3f}, F1: {best_f1:.3f}\")\n",
    "        else:\n",
    "            # Fallback to original F1 optimization if no threshold meets precision requirement\n",
    "            best_idx = np.argmax(f1_scores)\n",
    "            best_threshold = thresholds[best_idx]\n",
    "            best_precision = precisions[best_idx]\n",
    "            best_recall = recalls[best_idx]\n",
    "            best_f1 = f1_scores[best_idx]\n",
    "            \n",
    "            print(f\"\\nOptimal threshold for {name}: {best_threshold:.3f} (maximizing F1, no point met 30% precision requirement)\")\n",
    "            print(f\"At this threshold - Precision: {best_precision:.3f}, Recall: {best_recall:.3f}, F1: {best_f1:.3f}\")\n",
    "        \n",
    "        if verbose:\n",
    "            # Plot ROC curve\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            fpr, tpr, _ = roc_curve(y_val, y_val_prob)\n",
    "            plt.plot(fpr, tpr, lw=2, label=f'ROC curve (AUC = {val_auc:.3f})')\n",
    "            plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            plt.title(f'{name} ROC Curve (Validation)')\n",
    "            plt.legend(loc='lower right')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.show()\n",
    "            \n",
    "            # Plot Precision-Recall curve\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            plt.plot(recalls, precisions, lw=2, label=f'PR curve (AP = {val_avg_precision:.3f})')\n",
    "            plt.scatter(best_recall, best_precision, color='red', s=100, label=f'Optimal (F1 = {best_f1:.3f}, threshold = {best_threshold:.2f})')\n",
    "            plt.xlabel('Recall')\n",
    "            plt.ylabel('Precision')\n",
    "            plt.title(f'{name} Precision-Recall Curve (Validation)')\n",
    "            plt.legend(loc='lower left')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.show()\n",
    "            \n",
    "            # Plot threshold analysis\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(thresholds, precisions, 'b-', label='Precision')\n",
    "            plt.plot(thresholds, recalls, 'g-', label='Recall')\n",
    "            plt.plot(thresholds, f1_scores, 'r-', label='F1 Score')\n",
    "            plt.axvline(x=best_threshold, color='k', linestyle='--', label=f'Optimal threshold = {best_threshold:.2f}')\n",
    "            plt.xlabel('Threshold')\n",
    "            plt.ylabel('Score')\n",
    "            plt.title(f'{name} - Threshold Analysis')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.hist(y_val_prob, bins=50, color='skyblue', edgecolor='black')\n",
    "            plt.axvline(x=best_threshold, color='r', linestyle='--', label=f'Optimal threshold = {best_threshold:.2f}')\n",
    "            plt.xlabel('Predicted Probability')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.title(f'{name} - Probability Distribution')\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        y_test_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "        y_test_pred_default = best_model.predict(X_test)\n",
    "        y_test_pred_optimal = (y_test_prob >= best_threshold).astype(int)\n",
    "        \n",
    "        test_default_precision = precision_score(y_test, y_test_pred_default)\n",
    "        test_default_recall = recall_score(y_test, y_test_pred_default)\n",
    "        test_default_f1 = f1_score(y_test, y_test_pred_default)\n",
    "        \n",
    "        test_optimal_precision = precision_score(y_test, y_test_pred_optimal)\n",
    "        test_optimal_recall = recall_score(y_test, y_test_pred_optimal)\n",
    "        test_optimal_f1 = f1_score(y_test, y_test_pred_optimal)\n",
    "        \n",
    "        test_auc = roc_auc_score(y_test, y_test_prob)\n",
    "        test_avg_precision = average_precision_score(y_test, y_test_prob)\n",
    "        \n",
    "        print(\"\\nTest Set Results:\")\n",
    "        print(f\"Default threshold - Precision: {test_default_precision:.3f}, Recall: {test_default_recall:.3f}, F1: {test_default_f1:.3f}\")\n",
    "        print(f\"Optimal threshold - Precision: {test_optimal_precision:.3f}, Recall: {test_optimal_recall:.3f}, F1: {test_optimal_f1:.3f}\")\n",
    "        print(f\"ROC AUC: {test_auc:.3f}\")\n",
    "        print(f\"PR AUC: {test_avg_precision:.3f}\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"\\nClassification Report (Test Set, Optimal Threshold):\")\n",
    "            print(classification_report(y_test, y_test_pred_optimal))\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            cm = confusion_matrix(y_test, y_test_pred_optimal)\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "            plt.title(f'{name} Confusion Matrix (Test, Optimal Threshold)')\n",
    "            plt.ylabel('True Label')\n",
    "            plt.xlabel('Predicted Label')\n",
    "            plt.show()\n",
    "            \n",
    "            if hasattr(best_model, 'feature_importances_'):\n",
    "                plt.figure(figsize=(10, 8))\n",
    "                importances = best_model.feature_importances_\n",
    "                sorted_idx = np.argsort(importances)\n",
    "                plt.barh(range(len(sorted_idx)), importances[sorted_idx])\n",
    "                plt.yticks(range(len(sorted_idx)), np.array(X_train.columns)[sorted_idx])\n",
    "                plt.xlabel('Importance')\n",
    "                plt.title(f'{name} Feature Importance')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "        \n",
    "        # Store results for the current model\n",
    "        results['models'][name] = {\n",
    "            'model': best_model,\n",
    "            'best_params': grid.best_params_,\n",
    "            'optimal_threshold': best_threshold,\n",
    "            'validation': {\n",
    "                'precision': val_precision,\n",
    "                'recall': val_recall,\n",
    "                'f1': val_f1,\n",
    "                'roc_auc': val_auc,\n",
    "                'pr_auc': val_avg_precision\n",
    "            },\n",
    "            'test_default': {\n",
    "                'precision': test_default_precision,\n",
    "                'recall': test_default_recall,\n",
    "                'f1': test_default_f1\n",
    "            },\n",
    "            'test_optimal': {\n",
    "                'precision': test_optimal_precision,\n",
    "                'recall': test_optimal_recall,\n",
    "                'f1': test_optimal_f1,\n",
    "                'roc_auc': test_auc,\n",
    "                'pr_auc': test_avg_precision\n",
    "            },\n",
    "            'thresholds': {\n",
    "                'values': thresholds,\n",
    "                'precisions': precisions,\n",
    "                'recalls': recalls,\n",
    "                'f1_scores': f1_scores\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Update best model based on optimal test F1 score\n",
    "        if test_optimal_f1 > results['best_f1']:\n",
    "            results['best_model'] = name\n",
    "            results['best_threshold'] = best_threshold\n",
    "            results['best_f1'] = test_optimal_f1\n",
    "    \n",
    "    # Compare model performance across all evaluated models\n",
    "    if verbose:\n",
    "        model_names = list(results['models'].keys())\n",
    "        test_f1_scores = [results['models'][m]['test_optimal']['f1'] for m in model_names]\n",
    "        test_roc_scores = [results['models'][m]['test_optimal']['roc_auc'] for m in model_names]\n",
    "        test_pr_scores = [results['models'][m]['test_optimal']['pr_auc'] for m in model_names]\n",
    "        \n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.bar(model_names, test_f1_scores, color='lightgreen')\n",
    "        plt.title('F1 Score Comparison (Test)')\n",
    "        plt.ylim(0, 1.0)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.bar(model_names, test_roc_scores, color='skyblue')\n",
    "        plt.title('ROC AUC Comparison (Test)')\n",
    "        plt.ylim(0.5, 1.0)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.bar(model_names, test_pr_scores, color='salmon')\n",
    "        plt.title('PR AUC Comparison (Test)')\n",
    "        plt.ylim(0, 1.0)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    print(f\"\\n=== Summary ===\")\n",
    "    print(f\"Best model: {results['best_model']}\")\n",
    "    print(f\"Best threshold: {results['best_threshold']:.3f}\")\n",
    "    print(f\"Best F1 score on test set: {results['best_f1']:.3f}\")\n",
    "    \n",
    "    # Print the classification report for the best model on the test set\n",
    "    print(\"\\n=== Best Model's Classification Report on Test Set ===\")\n",
    "    best_model_obj = results['models'][results['best_model']]['model']\n",
    "    best_threshold = results['best_threshold']\n",
    "    y_test_prob_best = best_model_obj.predict_proba(X_test)[:, 1]\n",
    "    y_test_pred_best = (y_test_prob_best >= best_threshold).astype(int)\n",
    "    print(classification_report(y_test, y_test_pred_best))\n",
    "    \n",
    "    return results\n",
    "\n",
    "#############################################\n",
    "# (Optional) Function to compare SMOTE ratios\n",
    "#############################################\n",
    "def compare_smote_ratios(all_results):\n",
    "    \"\"\"\n",
    "    Compare model performance across different SMOTE ratios.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    all_results : Dictionary mapping SMOTE ratios to their evaluation results.\n",
    "    \"\"\"\n",
    "    ratios = sorted(all_results.keys())\n",
    "    best_f1_scores = [all_results[r]['best_f1'] for r in ratios]\n",
    "    best_models = [all_results[r]['best_model'] for r in ratios]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(ratios, best_f1_scores, 'o-', color='blue', markersize=8)\n",
    "    for i, model in enumerate(best_models):\n",
    "        plt.annotate(model, (ratios[i], best_f1_scores[i]), \n",
    "                     textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xlabel('SMOTE Ratio')\n",
    "    plt.ylabel('Best F1 Score (Test)')\n",
    "    plt.title('Effect of SMOTE Ratio on Best Model Performance')\n",
    "    if len(ratios) > 1:\n",
    "        z = np.polyfit(ratios, best_f1_scores, 1)\n",
    "        p = np.poly1d(z)\n",
    "        plt.plot(ratios, p(ratios), \"r--\", alpha=0.7, label=f\"Trend: y={z[0]:.4f}x+{z[1]:.4f}\")\n",
    "        plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for ratio in ratios:\n",
    "        res = all_results[ratio]\n",
    "        best_model_name = res['best_model']\n",
    "        m_res = res['models'][best_model_name]\n",
    "        recalls = m_res['thresholds']['recalls']\n",
    "        precisions = m_res['thresholds']['precisions']\n",
    "        pr_auc = m_res['test_optimal']['pr_auc']\n",
    "        plt.plot(recalls, precisions, lw=2, \n",
    "                 label=f\"Ratio {ratio} - {best_model_name} (AP = {pr_auc:.3f})\")\n",
    "    if 'baseline_fraud_rate' in all_results:\n",
    "        plt.axhline(y=all_results['baseline_fraud_rate'], color='r', linestyle='--', \n",
    "                  label=f'Baseline (fraud rate = {all_results[\"baseline_fraud_rate\"]:.3f})')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curves for Best Models at Different SMOTE Ratios')\n",
    "    plt.legend(loc='lower left')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "#############################################\n",
    "# Example usage:\n",
    "# Assuming you have already prepared your data splits:\n",
    "# X_train, X_val, X_test, y_train, y_val, y_test\n",
    "# And optionally, X_train_resampled, y_train_resampled from SMOTE.\n",
    "#############################################\n",
    "# For instance:\n",
    "# X_train, X_val, X_test, y_train, y_val, y_test, X_train_resampled, y_train_resampled = prepare_data(final_df, ...)\n",
    "\n",
    "model_results = evaluate_fraud_models(\n",
    "    X_train, X_val, X_test, \n",
    "    y_train, y_val, y_test,\n",
    "    X_train_resampled, y_train_resampled,\n",
    "    smote_ratio=0  # Adjust as needed\n",
    ")\n",
    "\n",
    "# If evaluating different SMOTE ratios, you can store results like this:\n",
    "all_results = {}\n",
    "all_results[1.0] = model_results\n",
    "# e.g., for another ratio:\n",
    "# model_results_0_5 = evaluate_fraud_models(..., smote_ratio=0.5)\n",
    "# all_results[0.5] = model_results_0_5\n",
    "# all_results['baseline_fraud_rate'] = y_test.mean()  # baseline for reference\n",
    "# compare_smote_ratios(all_results)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (en605645)",
   "language": "python",
   "name": "en605645"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
